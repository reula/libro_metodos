\chapter{Linear Algebra}


\section{Vector Spaces}

\defi:
A {\bf Real Vector Space} consists of three things:
--- $i$) a set, $V$, whose elements will be called {\bf vectors};
$ii$) a rule that assigns to each pair of vectors, $\ve{v}$,
$\ve{u}$, a third vector which we will denote by $\ve{v+u}$ and call
their {\bf sum}; and $iii$) a rule that assigns to each vector, $\ve{v}$ and
to each real number $a$, a vector which we will denote by $a\ve{v}$ and
call the {\bf product} of $\ve{v}$ with $a$, all subject to
the following conditions:

\begin{itemize}

\item[1.a)] For any pair of vectors $\ve{u}$, $\ve{v} \in V$
it holds that,
\beq
\ve{u} + \ve{v} = \ve{v} + \ve{u}
\eeq

\item[1.b)] There exists in $V$ a unique element called {\bf zero} and denoted by $\ve{0}$,
such that 

\beq
\ve{0+v = v} \;\forall \ve{v}  \in V.
\eeq

\item[1.c)] For any vector $\ve{u} \in V$ there exists a unique
vector in $V$, denoted $-\ve{u}$, such that,
\beq
\ve{u} + (-\ve{u}) = \ve{0}
\eeq

\item[2.a)] For any pair of real numbers $a$ and $a'$ and any vector 
$\ve{v}$ it holds that,
\[
a(a'\ve{v}) = (aa')\ve{v}.
\]

\item[2.b)] For any vector 
$\ve{v}$ it holds that,
\[
1\ve{v} = \ve{v}.
\]

\item[3.a)] For any pair of real numbers $a$ and $a'$ and any vector 
$\ve{v}$ it holds that,
\[
(a+a')\ve{v} = a\ve{v} + a'\ve{v}.
\]

\item[3.b)] For any real number $a$ and any pair of vectors
$\ve{v}$ and $\ve{v}'$ it holds that,
\[
a(\ve{v}  + \ve{v}') = a\ve{v} + a\ve{v}'.
\]

\end{itemize}

The first conditions involve only the rule of addition; these are actually the rules that 
define a group, a structure we will revisit in a later chapter, where addition represents the product between elements of the group.
The next conditions involve only the rule of multiplication, while
the last two deal with the relation between these two operations. As we will see with examples later, real numbers can be replaced by any field, such as rationals, $\Rationals$, integers, $\Integers$, complex numbers, $\Complex$, and even finite fields.

\espa

\ejem: The set of all $n$-tuples of real numbers with the
usual operations of component-wise addition and multiplication.
This space is denoted by $\re^n$.

\ejem: Let $S$ be any set and let $V$ be the set of all functions
from $S$ to the reals, $\ve{v}:S \to \re$, with the following
operations of addition and multiplication: the sum of the function $\ve{v}$ with
the function $\ve{v}'$ is the function (element of $V$) that assigns to the element
$s \in S$ the value $\ve{v}(s) + \ve{v}'(s)$. The product of
$a \in \re$ with the function $\ve{v}$ is the function that assigns to $s \in S$
the value $a\ve{v}(s)$.
This example will appear very often in the following chapters.

\defi: We will say that a set of vectors $\{\ve{e_i}\}$, $i=1, \ldots ,n$,
are {\bf linearly independent}~\index{linear!independence} 
if $\sum_{i=1}^{n} a^i \ve{e_i} = \ve{0}$ $a^i \in \re$ $\Longrightarrow $ $a^i = 0$, $i= 1, \ldots ,n$;
that is, if any non-trivial linear combination of these vectors gives us a non-zero vector. 

\defi: The {\bf span} of a set of vectors $\{\ve{u_{i}}\}$ is the set of all possible linear combinations of these elements. They generate a subspace of $V$, which we will denote by $Span\{\ve{u_{i}}\}\subset V$. 

If a finite number of linearly independent vectors, 
$n$, are sufficient to span $V$ (that is, if 
any vector in $V$ can be obtained as a linear combination of 
these $n$ vectors) then we will say that these vectors form a {\bf basis} of 
$V$ and that the {\bf dimension}~\index{dimension} of $V$ is $n$, $\dim V = n$.
\espa

\ejer: Show that given a vector $\ve{v} \in V$ and a basis of V, $\{\ve{e_i}\}$ $i=1, \ldots ,n$, the linear combination of the basis vectors that equals $\ve{v}$ is unique. That is, if 
$\ve{v} =  \sum_i v^i \ve{e_i} $ and $\ve{v} = \sum_i \tilde{v}^i \ve{e_i} $, then $v^i=\tilde{v}^i$ for all
$i=1, \ldots ,n$.

\ejer: If we have two bases of a real vector space $V$ of dimension $n$, $\{\ve{e_i}\}$ and $\{\ve{\tilde{e}_i}\}$, we can write the elements of one in terms of the other, 

\[
\ve{\tilde{e}_j} = \sum_i P_{j}{}^i \ve{e_i}, \;\;\;\;\;\; \ve{e_i} = \sum_l R_{i}{}^l \ve{e_l},
\]
%
where $P_{j}{}^i$ and $R_{i}{}^l$ are the coefficients of two real $n \times n$ matrices. Prove that $R_{i}{}^l = P^{-1}_{i}{}^l $.

\ejer:
Let $S = \{s_1,s_2,\dots,s_n\}$ be a finite set. Find a basis 
of the vector space of all functions from $S$ to $\re$. 
What is the dimension of this space?

\ejer: Show that the dimension of a vector space $V$ is well defined, i.e. it does not depend on
the basis used to define it. 

Note that, in the previous example, $V$ has finite dimension\footnote{That is, a finite number of linearly independent vectors span $V$.} because $S$ is finite. If $S$
had an infinite number of elements, $V$ would have an infinite basis and we would say that the dimension of $V$ would be infinite. In the rest of this Chapter, we will assume the vector spaces are finite-dimensional, unless otherwise stated.

Let $V$ be a real vector space of dimension $n$ and a basis of it, $\{\ve{e_i}\}$ $i=1, \ldots ,n$. 
Given an $n$-tuple of real numbers, $\{c^{i}\}$, we then have determined an element of $V$, 
that is, the vector,  $\ve{v}=\sum_{i=1}^{n} c^{i}\ve{e_i}$. 
On the other hand, we have seen in a previous exercise that, given any vector, it determines a unique $n$-tuple of real numbers, the coefficients of $\ve{v}$ in that basis. 
We see that we then have an invertible map between $V$ and the space of $n$-tuples, $\re^{n}$.
This map is linear, assigning to the sum of two vectors $\ve{v}$ and $\ve{\tilde{v}}$ the $n$-tuple sum of the respective $n$-tuples. This map depends on the basis, but it still tells us that finite-dimensional real vector spaces do not hold many surprises: they are always copies of $\re^{n}$.

\subsection{Covectors and Tensors}
\label{sub:Covectors_and_Tensors}

Let $V$ be a vector space of dimension $n$. 
Associated with this vector space, consider the set of linear maps from $V$ to $\re$,
$V^*=\{\ve{\omega}:V\to \re : \ve{\omega} \text{is linear}\}$. 
This is also a vector space, called the {\bf dual space to $V$}~\index{dual space}, 
or {\bf space of covectors},~\index{covectors} with addition and multiplication given by: 

\[
(\ve{\omega}+\alpha \ve{\tau})(\ve{ v})=
\ve{\omega}(\ve{ v})+\alpha \ve{\tau}(\ve{ v})\;\;\;\;\forall\;\ve{v}\in V
\]
%
with  $\ve{\omega},\ve{\tau}\,\in V^*\;,\,\alpha \in \re$. 

What is the dimension of $V^*$? 
Note that if
$\{\ve{ e}_i\}$ $i=1,\ldots,n$ is a basis of $V$, we can define $n$
elements $\ve{ \theta^i}  \in V^*$ by the relation 
\beq
\ve{ \theta^i}(\ve{ e}_j)=\delta^i_{\;j}.
\eeq
That is, we define the action of $\ve{ \theta^i}$ on the basis vectors $\ve{e_j}$
as in the equation above, and then extend its action to any vector in $V$ by writing this element in the basis $\{\ve{ e}_i\}$
and using the fact that the $\ve{ \theta^i}$ act linearly on vectors.
  
It can be easily seen that any covector $\rho \in V^*$ can be obtained
as a linear combination of the covectors $\{\ve{\theta^j}\}$,
$j=1,\ldots,n$ and that these are linearly independent, therefore
they form a basis and thus the dimension of $V^*$ is also $n$. We will call this basis the {\bf co-basis} of the basis $\{\ve{e_i}\}$.
\espa

\ejer: Prove that $V^*$ is a vector space and that the $\{\ve{ \theta^i}\}$
really form a basis.

\ejer: Prove that if $\ve{v} = \sum_{i=1}^{n} v^i \ve{e}_i$ then the coefficients can be obtained by acting on $\ve{v}$ with the covectors $\ve{\theta}^i$, that is,

\[
v^i = \ve{\theta}^i(\ve{v}).
\]

\ejer:
Let $V$ be the space of functions $S \to \re$, where $S$ is a finite set with $n$ elements.
Let a basis of $V$ be given by:

\[
\ve{e}_i (a) := \left\{
\begin{array}{cl}
1 & \text{if} \;\; a \;\; \text{is the i-th element of} S \\
0 & \text{otherwise}
\end{array}
\right.
\]
%
Find the corresponding co-basis of its dual space. 

Since $V$ and $V^*$ have the same dimension, they are, as vector
spaces, isomorphic. For any basis of $V$ have an associated co-basis of $V^*$, and we can extend this to associate to any vector in $V$ a covector in $V^*$.
However, this association is dependent on the choice of basis for $V$: if we choose a different basis, the association would be completely different. 
Then, this association is not intrinsic to the vector space. Since there is no \textit{natural} map that identifies $V$ with $V^*$, we have to consider them as intrinsically different, although isomorphic. 

What happens if we now take the dual of $V^*$? Will we get another intrinsically different vector space? The answer is negative, since \textit{there is} a natural identification
between $V$ and its double dual $V^{**}$.

Indeed, to each $\ve{v}\in V$ we can associate an element $\ve{
X}_v$ of
$V^{**}$ (that is, a linear functional from $V^*$ to $\re$) in the
following way: $\ve{X}_v(\omega):=\omega(\ve{v}) \;\;\;\forall
\;\;\omega\in V^{*}$. $\ve{X}_v(\omega)$ is sometimes called, unsurprisingly, the \textit{evaluation map} on $\ve{v}$. That is, the element $\ve{X}_v$ of $V^{**}$
associated with $\ve{ v}\in V$ is the one that, when acting on any
covector $\omega$, gives the number $\omega (\ve{v})$. Note that $\ve{X}_v$ acts
linearly on the elements of $V^*$ and therefore is an element
of $V^{**}$. Are there elements of $V^{**}$ that do not come from some
vector in $V$? The answer is negative, since the map  $\ve{X_v}:V\to V^{**}$ is
injective [$\ve{X}_v(\omega)=0\;\;\forall \; \omega\Longrightarrow \ve{v}=0$] and
therefore\footnote{Denoting by $\ve{X}_V$ the image by $\ve{X}_{(\cdot)}$ 
of $V$.} $\dim \ve{X}_V = \dim V$.
On the other hand $\dim V^{**} = \dim V^*$ since $V^{**}$ is the dual of $V^*$ and thus 
$\dim V = \dim V^* = \dim V^{**}$, which indicates that the map in question is also 
surjective and therefore invertible.
This allows us to \textit{identify}
$V$ and $V^{**}$ and conclude that by dualizing further we will not be able
to construct any more interesting vector spaces. In the case where the dimension of the vector space is not finite, this is no longer true and there are cases ---frequently used in physics--- where $\ve{X}_V \subset V^{**}$ strictly.
\espa

\ejer: Given a basis of $V$, $\{\ve{e_{i}}\}$, and the corresponding co-basis, $\{\ve{\theta^{j}}\}$, define the co-co-basis of $V^{**}$, $\{\ve{E_{i}}\}$. 
Find the relation between the components of a vector of the form $X_{v}$ in the basis $\{\ve{E_{i}}\}$ and 
those of the vector $\ve{v}$ in the basis $\{\ve{e_{i}}\}$.

\ejer: Prove that indeed $\dim \ve{X}_V = \dim V$.

However, nothing prevents us from also considering \index{multilinear maps}
{\bf multilinear maps}\footnote{That is, maps that are separately linear in each 
of their arguments.} 
from $\underbrace{V\times V\times\cdots\times V}_{k\;\mbox{times}}$ to $\re$,
or more generally,

\[
\underbrace{V\times\cdots\times V}_{k\;\mbox{times}}
\times\underbrace{ V^*\times \cdots\times V^*}_{l\;\mbox{times}} \to \re.
\]
%
The set of these maps, for each given pair $(k,l)$, is also a vector 
space with the obvious operations and its elements are called
{\bf tensors of type ${l\choose k}$}~\index{tensors!type}.

\ejer: What is the dimension of these spaces as a function of their type ${l \choose k}$?

\noi\yaya{Note}:
In finite dimension, it can be shown that any tensor of type ${l \choose k}$
can be written as a linear combination of elements of the Cartesian product of $k$ copies of $V^*$ and $l$ copies of $V$ 
---where we have identified $V$ with $V^{**}$---. 
For example, if $\ve{t}$ is of type ${0 \choose 2}$, ---that is, a map that has as arguments two covectors---, 
then given a basis $\{\ve{e_{i}}\}$ of $V$, and the corresponding basis of $V^{**}$, $\{\ve{E}_{i}\}$ there will be $n \times n$ real numbers $t^{ij}$, $i=1, \ldots ,n$ such that 
\beq
\ve{t}(\ve{\sigma},\ve{\omega}) = 
\sum_{i,j=1}^{n} t^{ij} \ve{E_i}(\ve{\sigma})\ve{E_j}(\ve{\omega})
=
\sum_{i,j=1}^{n} t^{ij} \ve{\sigma}(\ve{e_i}) \ve{\omega}(\ve{e_j}), 
\;\;\;\; \forall \ve{\sigma},\;\ve{\omega} \in V^*.
\eeq
%
But the set of linear combinations of Cartesian products of
$k$ copies of $V^*$ and $l$ copies of $V$ is also a vector
space, it is called the {\bf outer product}~\index{outer product} 
of  $k$ copies of $V^*$ and $l$ copies of $V$ and is denoted by 

\[
\underbrace{V^*\otimes V^*\otimes\cdots\otimes V^*}_{k\;\mbox{times}}
\otimes\underbrace{ V\otimes \cdots\otimes V}_{l\;\mbox{times}}.
\]
%
Therefore, tensors can also be considered as elements
of these outer products.

\ejem: a) Let $\ve{ t}$ be of type ${0 \choose 2}$, that is, $\ve{ t}\in\;V^*\otimes V^*$. 
This is a bilinear map from $V\times V$ to $\re$, $\ve{ t}(\ve{v},\ve{u})\in \re$. Let
$\ve{ t}$ be symmetric $[\ve{ t}(\ve{v},\ve{u})=\ve{ t}(\ve{u},\ve{v})]$ and non-degenerate 
[$\; \ve{ t}(\ve{v},\cdot)= 0
\in V^*\Longrightarrow \ve{v}=0$]. 
Since $\ve{ t}$ is non-degenerate, it defines an invertible map between $V$ and its dual. 
Indeed, given $\ve{v} \in V$, $\ve{t}(\ve{v},\cdot)$ is an element of $V^*$. 
But if $\ve{v}$ and $\ve{\tilde{v}}$ determine the same element of $V^*$, that is, if $\ve{ t}(\ve{v},\ve{u}) = \ve{ t}(\ve{\tilde{v}},\ve{u}) \;\; \forall \; \ve{u} \in V$ then $\ve{v}=\ve{\tilde{v}}$, which can be seen by taking $\ve{u} = \ve{v} - \ve{\tilde{v}}$ and using that $\ve{t}$ is non-degenerate. Since the dimensions of $V$ and $V^{*}$ are equal, the map thus defined is invertible.

\ejem: b) Let $\ve{\varepsilon}$ be a tensor of type ${0 \choose n}$ such that \beq \ve{\varepsilon} (\ldots,\underbrace{\ve{v}}_i,\ldots,\underbrace{\ve{u}}_j,\ldots) =-\ve{\varepsilon} (\ldots,\underbrace{\ve{u}}_i,\ldots,\underbrace{\ve{v}}_j,\ldots) \eeq \noi for any box $i$ and $j$, that is, a totally antisymmetric tensor. Let ${\ve{e}_i}$ be a basis of $V$ and $\varepsilon_{123\ldots n}:= \ve{\varepsilon} (\ve{e}_1,\ve{e}_2,\ldots,\ve{e}_n)$. Then any other component of $\ve{\varepsilon}$ in this basis will be either zero or $\varepsilon_{123\ldots n}$ or $-\varepsilon_{123\ldots n}$ depending on whether some $\ve{e}_i$ is repeated, or is an even permutation of the above, or an odd one. Indeed, for example,

\begin{eqnarray*} 
\varepsilon_{3124\ldots n} &:=& \ve_{\varepsilon}(\ve{e_3},\ve{e_1}, \ve{e_{2}}, \ve{e_{4}},\ldots,\ve{e}_n) \\
&=& -\ve{\varepsilon}(\ve{e_1},\ve{e_3}, \ve{e_{2}}, \ve{e_{4}},\ldots,\ve{e}_n) \\ 
&=& \ve{\varepsilon}(\ve{e_1},\ve{e_2}, \ve{e_{3}}, \ve{e_{4}},\ldots,\ve{e}_n) \\
&=&\varepsilon_{1234\ldots n}. 
\end{eqnarray*} 
% 
Therefore, given a basis, a single number, $\varepsilon_{123\ldots n}$, is enough to determine the tensor $\ve{\varepsilon}$ and given another tensor $\ve{\tilde{\varepsilon} }$ not identically zero with the properties mentioned above, there will exist a number $\alpha$ such that $\ve{\varepsilon} = \alpha\ve{\tilde{\varepsilon}}$. This last equality does not depend on the basis used and tells us that the dimension of the subspace of antisymmetric tensors of type ${0 \choose n}$ is 1. Knowing one element is enough to generate the entire space by multiplying it by any real number.

\ejer: Let $\ve{\varepsilon}$ be a non identically zero, totally antisymmetric tensor of type ${0 \choose n}$ and ${\ve{u}_i}$ a set of $n=\dim V$ vectors of $V$. Show that these form a basis if and only if 
\beq 
\ve{\varepsilon} (\ve{u_1},\ldots, \ve{u_n})\neq 0. 
\eeq

\ejem: c) Let $\ve{ A}$ be a tensor of type ${1 \choose 1}$, mapping
\beq
\ve{ u}\in V, \;\; \ve{ v}^* \in V^* \to \ve{A}(\ve{ u},\ve{v}^*)\in\re. 
\eeq 
This implies that $\ve{A}(\ve{u},\cdot)$ is also a vector ---identifying $V$ with $V^{**}$), the one that takes a covector $\ve{\omega} \in V^*$ and gives the number $\ve{A}(\ve{u},\ve{\omega})$---. 
Thus, $\ve{A}$ determines a \textbf{linear map} $V \to V$, that is, a linear operator on $V$.

\ejer: Continuing the example before, let ${\ve{u}_i}$ be a basis of $V$ and let $\ve{ a}_i = \ve{A}(\ve{u_i},\cdot)$. Then $$ \ve{\varepsilon}(\ve{a_1},\ldots,\ve{a_n})=\ve{\varepsilon}(\ve{A}(\ve{u_1}, \cdot),\ldots ,\ve{A}(\ve{u_n},\cdot)) $$ is totally antisymmetric in the ${u_i}$ and therefore proportional to itself;

\[ 
\ve{\varepsilon}(\ve{A}(\ve{u_1},\;\;),\ldots ,\ve{A}(\ve{u_n},\;\;)) \propto \ve{\varepsilon}(\ve{u_1}. \ldots, \ve{u_n}). 
\] % 

The proportionality constant is called the {\bf determinant} \index{determinant} of the operator $\ve{A}$,

\[ 
\ve{\varepsilon}(\ve{A}(\ve{u_1},;;),\ldots ,\ve{A}(\ve{u_n},;;)) =: \det(\ve{A}) \ve{\varepsilon}(\ve{u_1}. \ldots, \ve{u_n}).
\] %

\bpro Show that this definition does not depend on the $\ve{\varepsilon} $ used nor the basis and therefore gives truly a function on the space of operators $V \to \re$. \epro

\ejer: If $\ve{A}$ and $\ve{B}$ are two operators on $V$, then $\ve{A}\cdot \ve{B} (\ve{v}):= \ve{A}(\ve{B}(\ve{v}))$. Show that $\det(\ve{A}\ve{B})=\det(\ve{A}) \det(\ve{B})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Complexification}
\label{sub:Complexificacion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Another way to obtain vector spaces from a given one, say $V$, is by extending the field where the multiplication operation is defined, if this is possible. The most common case is the \textbf{complexification} of a real vector space; in this case, the product is simply extended to complex numbers, resulting in a vector space of the same dimension (over the complex field). One way to obtain it, for example, is by taking a basis of the real vector space $V$, and considering all linear combinations with arbitrary complex coefficients. The space thus obtained is denoted by $V^{\Complex}$. While the components of the vectors in $V$ in the original basis were $n$-tuples of real numbers, they are now $n$-tuples of complex numbers. Since the basis is the same, the dimension is also the same. These extensions of vector spaces often appear, and we will see others later.

Multilinear maps must be extended in the same way. That is, for example, the dual of $V$ will consist of all (complex-)linear maps from $V$ to $\mathbb{C}$.

We can also take smaller fields, for example, $\mathbb{Q}^n$ or $\mathbb{Z}^n$.

\ejem: Consider the vector space $\mathbb{Q}^n$ consisting of all $n$-tuples of rational numbers. In this space, the field is also the set of rationals. If we extend the field to the reals, we obtain $\ren$.

\ejem: Consider a space $V$ and any basis $\{\ve{e_{i}}\}$ of $V$. This choice characterizes a subspace of $V$, given by all elements of the form, 
\[ 
v=\sum_{i}^{n} m^{i}\ve{e_{i}} \;\;\;\;\; m^{i} \in Z. 
\]

\ejer: Now consider the set of all linear maps from this subspace to $Z$. What form do their elements take?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quotient Spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The last way we will see to obtain vector spaces from other vector spaces is by taking \textbf{quotients}. Let $V$ be a vector space and let $W \subset V$ be a subspace of it. We will call the \textbf{quotient space} the set of equivalence classes in $V$, where we will say that two vectors in $V$ are equivalent if their difference is a vector in $W$. This space is denoted as $V/W$.

\ejer: Prove that the relation defined above is indeed an equivalence relation. (At the end of the chapter there is a box with a discussion of the relevant definitions and properties of the central concept of equivalence relations.)

Let's see that this is a vector space. The elements of $V/W$ are equivalence classes; two elements of $V$, $\ve{v}$ and $\ve{v}'$, belong to the same equivalence class if $\ve{v}-\ve{v}' \in W$. Let $\ve{\zeta}$ and $\ve{\zeta}'$ be two elements of $V/W$, that is, two equivalence classes of elements of $V$. We will define the sum and the product of equivalence classes as follows: $\ve{\zeta} + \alpha \ve{\zeta}'$ will be the equivalence class corresponding to an element $\tilde{\ve{v}}$ obtained by taking a representative vector from $\ve{\zeta}$, say $\ve{v}$, another from $\ve{\zeta}'$, say $\ve{v}'$, and defining $\ve{\tilde{v}} := \ve{v} + \alpha \ve{v'}$, we have $\tilde{\ve{\zeta}}= \ve{\zeta} + \alpha \ve{\zeta}'$, where $\tilde{\ve{\zeta}}$ is the equivalence class containing the element $\ve{\tilde{v}} = \ve{v} + \alpha \ve{v'}$. To facilitate notation, the equivalence class containing a given element, say $\ve{v}$, is usually represented as $\[ \ve{v} \]$. In this case, we have,

\[ 
\[\ve{v}\] + \alpha \[ \ve{v'} \] = \[\ve{v} + \alpha \ve{v'}\]. 
\]

\ejer: See that this definition does not depend on the choice of representatives in the equivalence classes taken to perform the operation. That is, consider two other elements in $\ve{\zeta}$ and $\ve{\zeta}'$, say $\hat{\ve{v}}$ and $\hat{\ve{v}}'$, and see that with them you obtain an element in the same class as $\tilde{\ve{v}} = \ve{v} + \alpha \ve{v}'$.

\ejem: Let $V = \re{}^2$, that is, the space of 2-tuples of real numbers. Let $\ve{v}$ be any element. This element generates the one-dimensional space $W_{\ve{v}}$ consisting of all vectors of the form $\alpha \ve{v}$, for $\alpha \in \re$. The quotient space $V/W_{\ve{v}}$ is the space composed of lines parallel to $\ve{v}$. That is, each line is an element of the quotient space, and there is a notion of addition and scalar multiplication among them.

\begin{figure}[htbp] \begin{center} \resizebox{7cm}{!}{\myinput{Figure/m2_1}} \caption{Geometric interpretation of the quotient space.} \label{fig:2_1} \end{center} \end{figure}

\ejer: Let $V$ be the set of continuous functions from $\re$ to $\re$ and let $W$ be the subset of those that vanish on the interval $[0,1]$. See that this is a vector subspace. Consider the space $V/W$. What space can you associate with this?

\section{Norms}

\defi: A {\bf norm} in a vector space $V$ is a map $|\ve x|:V\to \re^+$, satisfying for all $\ve{x},\ve{y} \in V,\;\alpha\in \re,$,

$i$) Non-negativity: $|\ve x|\geq 0$, with equality only for $\ve x=0$

$ii$) Homogeneity: $|\alpha \ve x| =| \alpha|\;|\ve x|$

$iii$) Triangle inequality: $|\ve x+\ve y|\leq|\ve x|+|\ve y|$

\noi\yaya{Examples}: In $\re^2$ :

\noi a) $|(x,y)|:= \text{max}\{|x|,|y|\}$;

\noi b) $|(x,y)|_{2}:=\sqrt{x^2+y^2}$ (Euclidean norm);

\noi c) $|(x,y)|_{1}:=|x| +|y|$;

\noi d) $|(x,y)|_{p}:=(|x|^{p} + |y|^{p})^{\frac{1}{p}} \;\;\;\;\; p\geq 1$;

\noi e) Let $\ve{t}$ be a positive-definite symmetric tensor of type ${0 \choose 2}$ on a vector space $V$, that is $\ve{t}(\ve u,\ve v)=\ve{t}(\ve v,\ve u)$, $\ve{t}(\ve u,\ve u)\geq 0$ (with equality only for $\ve u=0$). The function $|\ve u|_{\ve{t}} =\sqrt{\ve{t}(\ve u,\ve u)}$ is a norm on $V$. Each tensor of this type generates a norm, but there are many norms that do not come from any tensor of this type. Give an example.

\ejer: Prove that $|\ve{t}(\ve{u},\ve{v})|^2 \leq |\ve{u}|_{\ve{t}} |\ve{v}|_{\ve{t}}$. Hint: Consider the polynomial: $P(\lambda) := \ve{t}(\ve{u} + \lambda \ve{v},\ve{u} + \lambda \ve{v})$.

\ejer: Prove that the given examples are indeed norms. Draw the level curves of the first four norms, that is, the sets $S_a=\{(x,y)\in \re^2\;/\; |(x,y)|=a\}$ and the "balls of radius $a$", that is $B_a=\{(x,y)\in \re^2/|(x,y)|\leq a\}$.

\ejer: Prove that the map $d:V\times V \to \re^+$ given by $d(\ve x,\ve y) = |\ve x-\ve y|$ for a norm $|\cdot|$ on $V$ defines a distance on $V$.

What is a norm geometrically? Given a vector $\ve{x}\neq 0$ of $V$ and any positive number, $a$, there is a unique number $\alpha > 0$ such that $|\alpha \ve x|=a$. This indicates that the level surfaces of the norm, that is, the hypersurfaces $S_a=\{\ve x\in V / |\ve x|=a\}$, $a>0$ form a smooth family of layers one inside the other, and each of them divides $V$ into three disjoint sets: the {\it interior} of $S_a$ ---containing the element $\ve x=0$---, $S_a$, and the {\it exterior} of $S_a$. The {\it interior} of $S_a$ is a convex set, that is, if $\ve x$ and $\ve y$ belong to the interior of $S_a$, then $\alpha \ve x+(1-\alpha)\ve y,\;\alpha\in[0,1]$ also belongs to it (since $|\alpha \ve x + (1-\alpha )\ve y| \leq \alpha |\ve x| + (1-\alpha )|\ve y| \leq \alpha a + (1-\alpha )a = a$).

A level set completely characterizes a norm in the sense that if we give a subset $N$ of $V$, such that $N$: \textbf{a)} has the radial property, that is, given $\ve x\neq 0$ there is a unique $\alpha >0$ such that $\alpha ,\ve x\in N$ and $-\alpha ,\ve x\in N$ and \textbf{b)} is convex, then there is a unique norm such that $N$ is the level surface $S_1$. This norm is defined as follows: given $\ve x$ we know that there will be a unique $\alpha > 0$ such that $\alpha \ve x \in N$ and then the norm of $\ve x$ will be $|\ve x| := \frac1{\alpha}$.

\ejer: Prove that this is a norm. Hint, given any two vectors $\ve{x},\;\; \ve{y} ; \in V$, then $\frac{\ve{x}}{||\ve{x}||}$ and $\frac{\ve{y}}{||\ve{y}||}$ are unitary and therefore are in $N$. But then we have that $||\lambda \frac{\ve{x}}{||\ve{x}||} - (1-\lambda)\frac{\ve{y}}{||\ve{y}||}|| \leq 1\;\;\;\forall \; \lambda \in [0,1]$. Now find a convenient value for $\lambda$.

From this perspective, we see that given two norms on a finite-dimensional vector space and a level surface of one of them, there will be level surfaces of the other norm that will have the former inside or outside of it. In the norms a) and b) of the previous example, we see that given a square containing zero, there will be two circles containing zero, one containing the square and the other contained by it. This leads us to the following theorem.

\bteo Let $V$ be a finite-dimensional vector space. Then all its norms are equivalent to each other, in the sense that given $|\cdot|$ and $|\cdot|'$ there are positive constants $M_1$ and $M_2$ such that for all $\ve x\in V$ it holds that $M_1|\ve x|\leq|\ve x|'\leq M_2|\ve x|$. \eteo

\pru: We will show that all are equivalent to the norm $|\ve x|_1=\sum_{i=1}^n |a^i|$, where the $a^i$ are the components of $\ve x$ with respect to a given basis $\{\ve{e}_i\}$, $ \ve x =a^i \,\ve{e}_i$.

Let $|\cdot|$ be any other norm, then

\beq
\barr{rcl} \left| \ve x| \right| & \leq & |\ve x|=|\sum_i^n a^i,\ve{e}_i| \leq \sum_i^n | a^i,\ve{e}i| \\ 
& \leq & \sum_i^n |a^i|,|\ve{e}i|\leq(max{j=1,n}|\ve{e}j|)\sum{i=1}^n|a^i| \\ 
& =&(max{j=1,n}|\ve{e}_j|);|\ve x|_1. 
\earr 
\eeq % 
And we have easily obtained the upper bound. Now let's see the lower bound. To do this, we must prove that the norm $| \cdot |$ is, as a function from $V$ to $\re^{+}$, a continuous function. This easily follows from the already found bound, indeed, let any other vector, $\ve y=b^i,\ve{e}i$, then 
\beq
\barr{rcl} \left|,|\ve x|-\|\ve y|,\right| & \leq & |\ve x-\ve y| \\ 
& =&(max{j=1,n}|\ve{e}_j|);|\ve x-\ve y|_1 
\earr 
\eeq

This shows that the norm $|\cdot|$ is a continuous function with respect to the norm $|\cdot|_1$. Let $S_1$ be the level surface of radius 1 with respect to the metric $|\cdot|_1$. $S_1$ is a closed and bounded set and therefore compact. Therefore, by continuity, $|\cdot|$ has a maximum value, $M_2$, and a minimum, $M_1$, which give the sought inequality. The maximum value we have already found, the minimum is what allows us to bound the norm from below and conclude the theorem.

\noi\yaya{Notes}:

\noi $i$) In this proof, it is crucial that $S_1$ is compact. If $V$ is infinite-dimensional, this is not the case, and there are many non-equivalent norms.

\noi $ii$) For our purposes, any norm is sufficient ---since if, for example, $f:V\to \re$ is continuous with respect to one norm, it is also continuous with respect to any other equivalent to it--- and for simplicity, from now on, we will use the Euclidean norm.

\noi $iii$) In this sense, the norms of finite-dimensional vector spaces are equivalent to the one generated by any positive-symmetric element of the outer product of its dual with itself.

\noi $iv$) Since equivalent norms generate the same topology, we see that in finite-dimensional vector spaces, there is a unique topology associated with all its possible norms. This is usually called the {\bf strong topology}.

\ejer: Prove that the above is indeed an equivalence relation among norms on a vector space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Induced Norms in $V^{\star}$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The norms defined in $V$ naturally induce norms in its dual, $V^{\star}$. This is given by: \begin{equation} \label{eq:norma_inducida_en_dual} |\ve{\omega}| := \max_{|\ve{v}|=1}{|\ve{\omega}(\ve{v})|}. \end{equation} 

In other words, the norm of a covector is the maximum value it takes on a unit vector.
\espa

\ejer: Show that this is a norm and that $|\ve{\omega}(\ve{v})| \leq |\ve{\omega}||\ve{v}|$.

\ejer: Consider $V=\re^2$ with the norm $|(x,y)|:= \max\{|x|,|y|\}$. What is the induced norm in $V^{\star}$?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Operator Theory}
\label{Teoria_de_Operadores_Lineales}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A {\bf linear operator} \index{operador!lineal} 
\ve{A} on a vector space $V$ is a continuous map\footnote{With respect to the topology induced by any of the equivalent norms of $V$.}
from $V$ to $V$ such that $\forall \; \ve x,\ve y \in V,\; \alpha \in \re$, 
$\ve{A}(\alpha \ve x+ \ve y)=\alpha\,\ve{A}(\ve x) + \ve{A}(\ve y)\;$. As we saw earlier, this is equivalent to saying that $\ve{A}$ is a tensor of type ${1 \choose 1}$.

The set of linear operators
$\cL$ is an algebra, that is, a vector space with a bilinear product among vectors.
Indeed, if $\ve{A}\,,\;\ve{B}\in \cL\,,\;\alpha\in \re$, 
then $\ve{A}+\alpha\ve{B}\in\cL$ and also 
$\ve{A}\cdot\ve{B}$ (the operator that sends $\ve x\in V$ 
to $A(B(\ve x)) \in V$) also belongs to $\cL$.
Due to this product among vectors, we can also define non-linear functions from $\cL$ to $\re$ and maps from $\cL$ to $\cL$. To study the continuity and differentiability of these maps, we introduce a norm in $\cL$, the most convenient being the following norm induced by the one used in $V$,
\beq
\|A\|_{\cL}=\mbox{max}_{\|\ve x\|_V=1}\|A(\ve x)\|_V.
\eeq
Similarly as defined for covectors, the norm of a linear operator is the maximum norm attained by applying it to a unit vector.

If $V$ is finite-dimensional (which we will assume from now on), 
the vector space $\cL$ is also finite-dimensional and therefore all its norms are equivalent. In this case, the norm of $A$ is finite because 
$A:V\to V$ is continuous and $\{\ve x\in V/\|\ve x\|_V=1\}$ is compact. However, in the infinite-dimensional case, neither of these statements is necessarily true, and within $\cL$ we only have a subspace of linear operators with finite (bounded) norms.
\espa

\ejer: Show that 
\[
\|\ve{A}(\ve{v})\| \leq \|\ve{A}\|_{\cL} \|\ve{v}\|.
\]
\espa

\ejer: Using the result of the previous exercise, show that 
\[
\|\ve{A}\ve{B}\|_{\cL} \leq \|\ve{A}\|_{\cL}\|\ve{B}\|_{\cL}.
\]
\espa

\ejer:
Show that $\|\;\|_{\cL}:\cL\to \re^+$ is a norm.
\espa

Next, we study various functions in the space of operators.
\espa

The determinant of an operator, introduced in the previous section, is a polynomial of degree $n$ = $\dim V$ in $\ve{A}$ and therefore differentiable. Using the chain rule, we see that
$\det (I+\lambda A)$ is differentiable in $\lambda$, and indeed a polynomial
of degree $n$ in $\lambda$. Each of the coefficients of this
polynomial is a function of $\ve{A}$. Of importance in what
follows is the linear coefficient in $\ve{A}$, which is obtained using the
formula
\beq
\der{\lambda} \det(\ve{I} + \left.\lambda \ve{A})\right|_{\lambda=0} =
\dip\frac{\eps (\ve{A}(u_1),u_2,\ldots,u_n)\,+\cdots
+\,\eps(u_1,\ldots, \ve{A}(u_n))}{\eps(u_1,\ldots,u_n)}
\label{eqn:2_traza}
\eeq
\noi This function is called the {\bf trace} \index{traza} of 
$\ve{A}$ and is denoted $\text{tr} (\ve{A})$. 

Among the maps from $\cL$ to $\cL$, consider the exponential map,
defined as,

\beq
e^{\dip \ve{A}}=\sum_{i=0}^{\infty}\frac{\ve{A}^i}{i!} =
\ve{I}+\ve{A}+\frac{\ve{A}^2}2 +\cdots
\eeq
\espa
\bteo
$e^{\dip\ve{A}}\in\cL$ if $\ve{A}\in \cL$ and $e^{t\ve{A}}$ is
infinitely differentiable with respect to $t$.
\eteo

\pru:
 Consider the Cauchy sequence $\{e^{\ve{A}}_n\}$, where
$e^{\ve{A}}_n \equiv\dip\sum_{i=0}^n \frac{\ve{A}^i}{i!}$. This
sequence is Cauchy because, taking $m > n$, we have

{\small
\begin{eqnarray}
\|e^{\ve{A}}_m - e^{\ve{A}}_n\|_{\cL} 
&=&
\|\frac{\ve{A}^{m}}{(m)!} + \frac{\ve{A}^{m-1}}{(m-1)!} + \frac{\ve{A}^{m-2}}{(m-2)!} + \dots 
+ \frac{\ve{A}^{n+1}}{(n+1)!} \|_{\cL} \nn
&\leq&
\|\frac{\ve{A}^{m}}{(m)!}\|_{\cL} +  \frac{\ve{A}^{m-1}}{(m-1)!}\|_{\cL} + \| \frac{\ve{A}^{m-2}}{(m-2)!} \|_{\cL}
+ \dots 
+ \|\frac{\ve{A}^{n+1}}{(n+1)!} \|_{\cL} \nn
&\leq& 
\frac{\|\ve{A}\|_{\cL}^{m}}{(m)!} +  \frac{\|\ve{A}\|_{\cL}^{m-1}}{(m-1)!} +  \frac{\|\ve{A}\|_{\cL}^{m-2}}{(m-2)!} 
+ \dots 
+ \frac{\|\ve{A}\|_{\cL}^{n+1}}{(n+1)!} \nn
&=& 
|e^{\|\ve{A}\|_{\cL}}_m - e^{\|\ve{A}\|_{\cL}}_n|
\to 0.
  \label{eq:2_exp}
\end{eqnarray}
}
%
Where 
$e^{\|\ve{A}\|_{\cL}}_n  \equiv \dip \sum_{i=0}^n \frac{\|\ve{A}\|^i_{\cL}}{i!}$ 
and the last implication follows from the fact that the numerical series
$e^{\|\ve{A}\|_{\cL}}$ converges. 
But by completeness\footnote{Every finite-dimensional real vector space is complete.} of $\cL$, every Cauchy sequence 
converges to some element of $\cL$ that we will call $e^{\ve{A}}$. 
The differentiability of $e^{t\ve{A}}$ follows from the fact that if a series
$\sum_{i=0}^{\infty}\;f_i(t)$ is convergent and
$\sum_{i=0}^{\infty}\;\dip\derc{f_i}{t}$ is uniformly
convergent, then $\der{t}\sum_{i=0}^{\infty}\,
f_i(t)=\sum_{i=0}^{\infty}\der{t} f_i(t)$
\epru

\espa
\ejer: 
Show that 

\noi a) $e^{(t+s)\ve{A}}=e^{t\ve{A}}\cdot e^{s\ve{A}},$

\noi b) If $\ve{A} $ and $\ve{B}$ commute, that is if $\ve{A}\ve{B} = \ve{B}\ve{A}$, then
$e^{\ve{A} + \ve{B}} = e^{\ve{A}}\;e^{\ve{B}}.$

\noi c) $\det(e^{\ve{A}})=e^{\text{tr}(\ve{A})}$.

\noi d) $\dip\der{t} e^{t\ve{A}}=\ve{A}\,e^{t\ve{A}}.$

\noi Hint: For point c) use that $e^{\ve{A}}$ can also be defined as,
\[
e^{\ve{A}} = \dip \lim_{m\to\infty} \left(\ve{I} + \frac{\ve{A}}m\right)^m.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Matrix Representation}
\label{Representacion_Matricial}

To describe certain aspects of linear operators, it is convenient 
to introduce the following matrix representation. \index{matrices!representación matricial}

Let $\{\ve{u}_i\}$, $i=1,\ldots,n$ be a basis of $V$, that is, a
set of linearly independent vectors of $V$ 
[$ \sum_{i=1}^n c^i\,\ve{u}_i=0 \Longrightarrow c^i=0$] 
that span it 
[if $\ve{v} \in V$, there exist numbers 
$\{v^i\}$, $i=1,\ldots,n$ such that $\ve{v}= \sum_{i=1}^n v^i\,\ve{u}_i)$]. 
Applying the
operator $\ve{A}$ to a member of the basis $\ve{u}_i$ we obtain a vector
$\ve{A}(\ve{u}_i)$ which in turn can be expanded in the basis,
$\ve{A}(\ve{u}_i)= \sum_{j=1}^n A^j{}_i\,\ve{u}_j$. 
The matrix thus constructed, $A^j{}_i$, is a
representation of the operator $\ve{A}$ in that basis. 
In this language,
we see that the matrix $A^j{}_i$ transforms the vector of components
$\{v^i\}$ into the vector of components $\{A^j{}_iv^i\}$.
Given a basis, $\{\ve{u}_i\}$, and a matrix $A^j{}_i$, we can construct a
linear operator as follows: Given the basis, we define its
co-basis, that is, a basis in $V^*$ as $\{\ve{\theta}^i\}$,
$i=1,\ldots,n$, such that $\ve{\theta}^i(\ve{u}_j)=\delta^i_j$, then $\ve{A} =
\sum_{i,j=1}^n A^j{}_i\, \ve{u}_j\ve{\theta}^i$.

If we change the basis, the matrices representing the operators
will change. Indeed, if we take another basis $\{\hat{\ve{u}}_i\}$ and
write its components with respect to the previous basis as  
$\hat{\ve{u}}_i= P^{k}{}_i\ve{u}_k$, and therefore, $\hat{\theta}^{j} = (P^{-1})^{j}{}_{l}\theta^{l}$, 
then the relation between the components of the operator $\ve{A}$ in both bases is given by
\beq
\hat A^j{}_i= A(\hat{\theta}^{j},\hat{\ve{u}}_i) = (P^{-1})^{j}{}_{l} \,A^l{}_k\,P^k{}_i\;\;\;\;\mbox{o}\;\;\;\;\hat A=
P^{-1}\,A\,P
\eeq
\noi that is, $\hat A$ and $A$ are \textbf{similar matrices}.
\espa

\ejer: See, from its definition (equation (\ref{eqn:2_traza})), that in a basis we have,
$tr{A}= \sum_{i=1}^n A^i{}_i$.

\ejer: See with an example in two dimensions that the definition of determinant conforms with the usual one when we use a basis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Invariant Subspaces}
\label{Subespacios_Invariantes}

\defi: Let $\ve{A}: V \to V$ be an operator and let $W$ be a subspace of $V$. 
We will say that $W$ is an \textbf{invariant subspace}~\index{subespacio invariante} 
of $\ve{A}$ if $\ve{A}W \subseteq W$.

The invariant subspaces of an operator are important because they allow us to understand its action.
Note that given any operator $\ve{A}$, there are always at least two invariant spaces, $V$ and $\{\ve{0}\}$, and in reality, many more. For example, as we will see later, given any number between $1$ and $n$ (=$\dim V$), there exists an invariant subspace with that number as its dimension. The ones that truly encode the action of the operator are its 
\textbf{irreducible}~\index{subespacio invariante!irreducible} 
invariant subspaces, that is, those that cannot be further decomposed into invariant subspaces such that their direct sum is the whole $V$~\footnote{A vector space is said to be the direct sum of two of its subspaces, $W_1$ and $W_2$, and is denoted by $V=W_1 \oplus W_2$ if each element of $V$ can be uniquely written as the sum of two elements, one from each of these subspaces.}
\espa

\ejem: Let $V$ with $\dim(V)=2$ and $\ve{A}: V \to V$ given by, $\ve{A}(\ve{u_1}) = \lambda_1 \ve{u}_1$,
$\ve{A}(\ve{u_2}) = \lambda_2 \ve{u}_2$, where $(\ve{u}_1, \ve{u}_2)$ are linearly independent (and therefore a basis). Note that this completely defines the operator, since given any $\ve{v} \in V$, we can uniquely write it as $\ve{v} = v^1 \ve{u}_1 + v^2 \ve{u}_2$ and therefore, 
\[
\ve{A}(\ve{v}) = \lambda_1 v^1 \ve{u}_1 + \lambda_2 v^2 \ve{u}_2.
\]
In this case, the invariant subspaces are $Span\{\ve{u}_1\}$ and $Span\{\ve{u}_2\}$, and clearly, we have 
$V= Span\{\ve{u}_1\} \oplus Span\{\ve{u}_2\}$. Since each of these invariant subspaces is one-dimensional, the action of the operator on them is simply a dilation, that is, the multiplication of their elements by a number. Note that in the case where $\lambda_1=\lambda_2$, the operator is proportional to the identity and therefore we have infinite invariant spaces.

\ejer: Let $V$ be the space from the previous example and let $\ve{A}$ be given by $\ve{A}(\ve{u}_1) = 0$,  $\ve{A}(\ve{u}_2) = \ve{u}_1$. Find its irreducible invariant subspaces. Do the same for the operator given by 
$\ve{A}(\ve{u}_1) = \ve{u}_1$,  $\ve{A}(\ve{u}_2) = a \ve{u}_2 + \ve{u}_1$. What happens when $a=1$?

We will study in detail the one-dimensional invariant subspaces, note that they are irreducible. 
To study the invariant spaces, it is convenient to study the invariant subspaces of the operator when its action is extended to $V^{\Complex}$, that is, the 
\textbf{complexification}~\index{complexificación} of $V$.

Let's see that an operator always has at least one one-dimensional invariant subspace (and therefore always has a non-trivial irreducible invariant subspace).

\begin{lem}
Given $\ve{A}: V^{\Complex} \to V^{\Complex}$, where $V^{\Complex}$ is finite-dimensional, there always exists a $\ve{u} \in V^{\Complex}$
and a $\lambda \in \Complex$ such that,
\beq
(\ve{A}-\lambda \ve{I})\ve{u}=0   
\label{eqn:2_av_av}
\eeq
\end{lem}

\pru:

A solution to this equation consists of a scalar $\lambda$, called the {\bf eigenvalue}~\index{autovalor} of
the operator $\ve{A}$, and a vector $\ve{u}$, called the {\bf eigenvector}~\index{autovector}
of the operator $\ve{A}$. The subspace of $V^{\Complex}$ given by 
$\{\alpha \ve{u}\;|\; \alpha \in \Complex\}$ is the invariant subspace sought.

It is clear that the system has a solution if and only if $ \det(\ve{A}-\lambda\ve{I})=0$. But this is a polynomial in
$\lambda$ of order equal to the dimension of $V$ and therefore, by the Fundamental Theorem of Algebra, it has
at least one solution or root, (generally complex), $\lambda_1$, 
and therefore there will be, associated with it, 
at least one $\ve{u}_1$ solution of (\ref{eqn:2_av_av}) with $\lambda = \lambda_1$
\epru
\espa
The need to consider all these solutions is what leads us to treat the problem for complex vector spaces. 

\ejer: In the infinite-dimensional case, the theorem is no longer true. Find an example of an operator on the set of infinite tuples without any eigenvector. Find it by looking for infinite matrices constructed in such a way that they have only one eigenvector and consider the limit to infinite components.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Application: Schur's Triangulation Lemma}
\label{subsub:Aplicacion:_Lema_de_triangulacion_de_Schur}

\defi: An $n \times n$ matrix $A^j{}_i$ has an upper triangular form if 
$A^j{}_i = 0 \;\; \forall j>i, \; j,i=1,\ldots, n$. 
That is, it is a matrix of the form,

\beq 
A=
\left(\barr{ccccc}
            A^{1}{}_{1}   & A^{1}{}_{2}  & \cdots  & \cdots & A^{1}{}_{n}    \\
            0        & A^{2}{}_{2}  & \cdots   & \cdots & A^{2}{}_{n}    \\
            0        &   0     & \ddots  & \ddots & A^{3}{}_{n}   \\
            \vdots   & \ddots  & \ddots  & \ddots & \vdots  \\
            0        & \cdots  & \cdots  & 0      & A^{n}{}_{n}
            \earr\right).
\label{2.S1}
\eeq 

As we will see later, in chapter \ref{Sistemas_Lineales},
this is a very convenient form to understand the solutions
to systems of ordinary differential equations. And the most attractive thing about it
is that any operator has a matrix representation with an upper triangular form! Moreover, if an inner product is present, the basis
for this representation can be chosen to be orthonormal.

\begin{lem}[Schur]
\label{2_lem_Schur}
Let $\ve{A}:V\to V$ be a linear operator acting on a finite-dimensional complex vector space $V$, $n$, and let $(\cdot,\cdot)$ be an inner product on $V$. Then, there exists an orthonormal basis 
$\{\ve{u}_i\},\; i=1,\ldots,n$ with respect to which the matrix representation of $\ve{A}$ is upper triangular.
\end{lem}

\pru: 
Consider the eigenvalue-eigenvector problem for $\ve{A}$,
\begin{equation}
(\ve{A} - \lambda \ve{I})(\ve{u}) = \ve{0}.
\end{equation}
%
As we have already seen, this problem always has at least one non-trivial solution, and therefore we have a pair $(\lambda_1,\ve{u}_1)$ solution
of the problem. 
We take $\ve{u}_1$ of unit norm as the first element
of the basis to be determined. 
We then have 
\[
A^j{}_1 := \ve{\theta}^j(\ve{A}(\ve{u}_1))
         = \ve{\theta}^j(\lambda_1 \ve{u}_1)
         = \lambda_1 \delta^j{}_1,
\]         
%
which gives us the result for the first column of the matrix. 

Now consider the space 
\[
V_{n-1} = Span\{\ve{u}_1\}^{\perp} := \{\ve{u} \in V | (\ve{u}_1,\ve{u}) = 0 \}
\]
and the operator from $V_{n-1} \to V_{n-1}$ given by $\ve{A}_1 :=(\ve{I} - \ve{u}_1 \ve{\theta}^1)\ve{A}$.
%
Note that as we form an orthonormal basis, we already know the first member of the co-basis, $\ve{\theta}^1 = (\ve{u}_1,\cdot)$.
%
The operator $P_1:=\ve{I} - \ve{u}_1 \ve{\theta}^1$ satisfies $P_1(\ve{u}_1)=0$, 
$(P_1(\ve{v}),\ve{u}_1)=0$ and $P_1\cdot P_1 = P_1$, that is, it is a projection operator in the subspace $V_{n-1}$.

We then have that $\ve{A}_1 : V_{n-1} \to V_{n-1}$. Therefore, in this space, we can also pose the eigenvalue-eigenvector equation, 

\begin{equation}
(\ve{A}_1 - \lambda \ve{I})\ve{u} = ((\ve{I} - \ve{u}_1 \ve{\theta}^1)\ve{A} - \lambda \ve{I})\ve{u} = \ve{0}.
\end{equation}
%

We thus obtain a new pair $(\lambda_2,\ve{u}_2)$, with $\ve{u}_2 \;\in V_{n-1}$, and therefore perpendicular
to $\ve{u}_1$ and also,
$\ve{A} \ve{u}_2 = \lambda_2 \ve{u}_2 + \ve{u}_1 \ve{\theta}^1(\ve{A}(\ve{u}_2))$.
Therefore 
\[
A^j{}_2 = \ve{\theta}^j(\ve{A}(\ve{u}_2))
         = \ve{\theta}^j(\lambda_2 \ve{u}_2 + \ve{u}_1 \ve{\theta}^1(\ve{A}(\ve{u}_2)))
         = \lambda_2 \delta^j{}_2 + \delta^j{}_1 A^1{}_2.
\]
%
We thus see that with this choice of basis element, the second column of $A$ satisfies the condition of the Lemma.
The next step is to consider the subspace,

\[
V_{n-2} = Span\{\ve{u}_1, \ve{u}_2\}^{\perp}
        = \{\ve{u} \in V | (\ve{u}_1,\ve{u}) = (\ve{u}_2,\ve{u}) = 0 \},
\]
%
and there the eigenvalue-eigenvector equation for the operator
$(\ve{I} - \ve{u}_1 \ve{\theta}^1 - \ve{u}_2 \ve{\theta}^2)\ve{A}$.
Proceeding in this way, we generate the entire basis\epru
\espa

The previous proof used an inner product that is actually exogenous to the property itself.
We did it this way because the proof is conceptually simple and useful in the case that we are in the presence of a given inner product.
However, the previous theorem can be proven using the notion of quotient space and thus dispensing with the inner product. 
This proof can be obtained by doing the following exercises:

\ejer: Let $\ve{A}: V \to V$ be a linear operator, let $W\subset V$ be invariant under $\ve{A}$, that is, $\ve{A}[W]\subset W$.
This operator induces an operator in the quotient space $V/W$ as follows: 
$\ve{\hat{A}}\zeta = \tilde{\zeta}$ where $\tilde{\zeta}$ is the equivalence class to which $\ve{A}(\ve{u})$ belongs when $\ve{u}$ belongs to $\zeta$. See that this definition is consistent, that is, if we choose another element $\ve{v} \in \zeta$, we obtain that $\ve{A}(\ve{v}) \in \tilde{\zeta}$.

\ejer: The induced operator will have at least one eigenvalue-eigenvector pair in $V/W$, that is, there will be a pair
$(\hat{\lambda}, \zeta)$ such that $\ve{\hat{A}\zeta} = \hat{\lambda}\zeta$. What does this equation mean in terms of the operator $\ve{A}$ in $V$?

\ejer: Specialize the previous case when $W$ is the subspace of $V$ generated by an eigenvector of $\ve{A}$.
$\ve{A}\ve{u}_{1} = \lambda_{1} \ve{u}_{1}$, $W_{1} = Span\{u_{1}\}$. 
What does it mean, in terms of the space $V$ and the operator $\ve{A}$, that $\ve{\hat{A}}:V/W_{1} \to V/W_{1}$ has an eigenvalue-eigenvector pair?

\ejer: Iterate the previous procedure, taking at each step an element from each equivalence class of generalized eigenvectors to form a basis where the matrix representation of $A$ is upper triangular.

\espa

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Improve..
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Continuing now with the study of invariant subspaces.
If $\det(\ve{A}-\lambda\ve{I})$ has $1\leq m \leq n$ distinct roots, 
$\{\lambda_i\}, \; i=1,\ldots,m$, then there will be at least one complex eigenvector 
$\ve{u}_i$ associated with each of them. 
Let's see that these form distinct invariant subspaces.

\espa
\blem
Let $\{(\lambda_i,\ve{u}_i)\}\;\;i=1\ldots m$ be a set of eigenvalue-eigenvector pairs.
If $\lambda_i\neq\lambda_j\;\;\;\;\forall\;i\neq j, \; i,j=1\ldots m$, then 
these eigenvectors are linearly independent.
\elem
\espa

\pru:
Suppose by contradiction that they are not, and therefore there exist constants 
$c^i\in\ve{C}$, $i=1,\ldots,m-1\,$, such that
\beq
\ve{u}_m=\sum_{i=1}^{m-1}\; c^i\,\ve{u}_i   \label{2_***}
\eeq
%
Applying $\ve{A}$ to both sides, we get
\beq
\ve{A}\ve{u}_m=\lambda_m\:\ve{u}_m=\sum_{i=1}^{m-1}\;c^i\,\lambda_i\,\ve{u}_i
\eeq
or,
\beq
0=\sum_{i=1}^{m-1}\;c^i\,(\lambda_m - \lambda_i)\,\ve{u}_i.
\eeq
We conclude that $\{\ve{u}_i\}\;\;i=1,\ldots,m-1$
are linearly dependent. Due to~\ref{2_***} and the hypothesis that the
eigenvalues are distinct, at least one of the coefficients must be non-zero,
and therefore we can solve for one of the remaining eigenvectors in terms of the others $m-2$. Repeating this procedure
$(m-1)$ times, we arrive at the conclusion that $\ve{u}_1=0$, which is a
contradiction since, as we have seen, the eigenvector equation always has
a non-trivial solution for each distinct eigenvalue\epru
\espa

If for each eigenvalue there is more than one eigenvector, then these form a higher-dimensional invariant vector subspace (reducible). Within each of these subspaces, we can take a basis composed of eigenvectors. The previous lemma ensures that all these eigenvectors thus chosen, for all eigenvalues, form a large linearly independent set.

\ejer: Convince yourself that the set of eigenvectors with the same eigenvalue forms
a vector subspace.

If a given operator $\ve{A}$ has all its eigenvalues distinct, then the corresponding eigenvectors are linearly independent and equal in number to the dimension of $V$, that is, they generate a basis of $V^{\Complex}$. 
In that basis, the matrix representation of $\ve{A}$ is diagonal, that is, $A^j{}_i = \delta^j{}_i \lambda_i$. 
Each of its eigenvectors generates an irreducible invariant subspace,
and together they generate $V^{\Complex}$. In each of them, the operator $\ve{A}$
acts merely by multiplication by $\lambda_i$. Note that the $\lambda_i$ are
generally complex, and therefore such multiplication is actually a rotation
plus a dilation. Note that, unlike the basis of Schur's triangulation lemma,
this is not generally orthogonal with respect to any given inner product.~\footnote{Note, however, that it can be \textsl{declared} orthogonal by defining
the inner product as 
$(\ve{u},\ve{v}) = \sum_{i=1}^{n} \ve{\theta}^i(\bar{\ve{u}})\ve{\theta}^i(\ve{v})$}
\espa

\ejem: Let $V$ be a vector space with $\dim(V)=2$ and let 
$\ve{A}$ be given by $\ve{A}(\ve{e}_1) = \ve{e}_2$, $\ve{A}(\ve{e}_2) = -\ve{e}_1$, where $(\ve{e}_1,\ve{e}_2)$ are any two linearly independent vectors. If we interpret them as two orthonormal vectors, then $A$ is a rotation by $\pi/2$ in the plane. 

Now let's calculate the determinant of $\ve{A} - \lambda \ve{I}$,

\begin{eqnarray}
\det(\ve{A}- \lambda\ve{I}) 
 & = &
\eps((\ve{A}- \lambda\ve{I})\ve{e}_1,(\ve{A}- \lambda\ve{I})\ve{e}_2)/
\eps(\ve{e}_1,\ve{e}_2) \nn
 & = &
\eps(\ve{e}_2-\lambda \ve{e}_1,-\ve{e}_1-\lambda \ve{e}_2)/
\eps(\ve{e}_1,\ve{e}_2) \nn
 & = &
 1 + \lambda^2.
\end{eqnarray}                        
%
and therefore the eigenvalues are $\lambda_{1} = \imath$, and $\lambda_2 = -\imath$.
The eigenvectors are $\ve{u}_1= \ve{e}_1+\imath \ve{e}_2$ and $\ve{u}_2=\ve{e}_1 - \imath \ve{e}_2 = \bar{\ve{u}}_1$. 
We see then that the action of $\ve{A}$ in these subspaces is multiplication by 
$\pm \imath$ and that both invariant subspaces are genuinely complex.
In this new basis, the space $V$ is generated by all linear combinations of
the form $z\ve{u}_1 + \bar{z}\ve{u}_2$, and the action of $\ve{A}$ is simply 
multiplication by $\imath$ of $z$.
\espa

If the multiplicity of any of the roots $\det(\ve{A}-\lambda\ve{I})=0$ is 
greater than one, there will be fewer eigenvalues than the dimension of the space, and therefore we will not be guaranteed to have enough eigenvectors to form a basis, as we can only guarantee the existence of one for each eigenvalue.

\ejem: Let $V$ be the set of 2-tuples of real numbers with a generic element
$(a,b)$ and let $\ve{A}$ be given by $\ve{A}(a,b) = (\lambda a+\epsilon b,\lambda b)$. Taking a basis, 
$\ve{e}_1=(1,0)$, 
$\ve{e}_2=(0,1)$, we see that its matrix representation is:

\beq\left(\barr{cc}
     \lambda & \epsilon    \\
     0 &  \lambda
     \earr\right)
\eeq
%
We see that this operator has only one eigenvalue with multiplicity 2. But it has only one eigenvector, proportional to $\ve{e}_1=(1,0)$. For future use, note that if we define 
$\ve{\Delta} = \ve{A}-\lambda \ve{I}$, then $\ve{e}_{1} = \frac{1}{\epsilon}\ve{\Delta}\ve{e}_2$,
and therefore, in the basis $\{\ve{\tilde{e}}_{1} = \ve{e}_{1}, \ve{\tilde{e}}_{2} = \frac{1}{\epsilon}\ve{e}_{2}\}$,
the operator is represented by the matrix,

\beq\left(\barr{cc}
     \lambda & 1    \\
     0 &  \lambda
     \earr\right)
\eeq

\espa

We must therefore analyze what happens in these cases. 
To do this, let us define, 
given $\lambda_i$ an eigenvalue of $\ve{A}$, the 
following subspaces:

\begin{equation}
  \label{eq:W_lm}
  W_{\lambda_i}{}_p = \{ \ve{u} \in V | \; (\ve{A} - \lambda_i \ve{I})^p \ve{u} = 0 \}
\end{equation}

Note that these are invariant spaces: 
$\ve{A} W_{\lambda_i}{}_p \subset W_{\lambda_i}{}_p$.
Moreover, $W_{\lambda_i}{}_p \subset W_{\lambda_i}{}_{p+1}$, and therefore, for a sufficiently large $p$ ($p \leq n$), we will have that 
$W_{\lambda_i}{}_p = W_{\lambda_i}{}_{p+1}$, taking the minimum among the
$p$'s where this occurs, we define $W_{\lambda_i} := W_{\lambda_i}{}_p$.
Note that if for some $\lambda_i$, $p=1$, then the subspace $W_{\lambda_i}$
is composed of eigenvectors.
These are the maximum invariant spaces associated with the eigenvalue $\lambda_i$,
indeed we have:

\blem
The only eigenvalue of $\ve{A}$ in $W_{\lambda_i}$ is $\lambda_i$.
\elem

\pru: Let $\lambda$ be an eigenvalue of $\ve{A}$ in $W_{\lambda_i}$. 
Let's see that $\lambda = \lambda_i$. 
As we have already seen, there will be an eigenvector $\ve{\zeta} \in W_{\lambda_i}$ 
with $\lambda$ as the eigenvalue.
Since it is in $W_{\lambda_i}$, there will be some $p \geq 1$ such that 
$(\ve{A} - \lambda_i \ve{I})^p \ve{\zeta} = 0$, but since it is an eigenvector, we have that
$(\lambda - \lambda_i)^p \ve{\zeta} = 0$, and therefore $\lambda = \lambda_i$
\epru
\espa

Now let's see that these subspaces are linearly independent and generate
all of $V^{\Complex}$. 
We will prove this theorem again in Chapter \ref{Sistemas_Lineales}.

\bteo[See Chapter \ref{Sistemas_Lineales}, Theorem \ref{5_teo_2}]
Given an operator $A: V \to V$, with eigenvectors $\{ \lambda_i \}, \; i=1...m$, 
the space $V^{\Complex}$ admits a direct decomposition into invariant subspaces 
$W_{\lambda_i}$,
where in each of them $\ve{A}$ has only $\lambda_i$ as an eigenvalue.
\eteo

\pru:

The $W_{\lambda_i}$ are independent. 
Let $\ve{v}_1 + \dots + \ve{v}_s = 0$, with
$\ve{v}_i \in W_{\lambda_i}$, then we must prove that each $\ve{v}_i=0$.
Applying 
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}$
to the previous sum, we get,
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}\ve{v}_1 =0$,
but since $\lambda_i$, $i \neq 1$, is not an eigenvalue of $\ve{A}$ in $W_{\lambda_1}$,
the operator 
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}$ is 
invertible~\footnote{
Note that $(\ve{A} - \lambda_i \ve{I})^{s}|_{W_{\lambda_j}}$ 
is invertible if its determinant is non-zero. 
But 
$\det (\ve{A} - \lambda_i \ve{I})^{s} = (\det(\ve{A} - \lambda_i \ve{I}))^s 
= (\lambda_j - \lambda_i)^{s \dim (W_{\lambda_j})} \neq 0$
}
in that subspace, and therefore $\ve{v}_1 =0$. Continuing in this way, we see that all
the $\ve{v}_i$ must be zero.

The $W_{\lambda_i}$ generate all of $V^{\Complex}$. Suppose by contradiction that this
is not the case, and consider $V^{\Complex}/W$, where $W$ is the space generated by all
the $W_{\lambda_i}$, that is, the space of all linear combinations of 
elements in the $W_{\lambda_i}$. The operator $\ve{A}$ acts on $V^{\Complex}/W$
[$\ve{A}\{\ve{u}\} = \{\ve{A}\ve{u}\}$], and therefore it has an eigenvalue-eigenvector pair there. This implies that for some element $\ve{\zeta}$ of $V^{\Complex}$ 
in some equivalence class of $V^{\Complex}/W$, we have:
\begin{equation}
  \ve{A}\ve{\zeta} = \lambda \ve{\zeta} + \ve{u}_1 + \dots + \ve{u}_s
\end{equation}
%
where the $\ve{u}_i$ belong to each $W_{\lambda_i}$.
Now suppose that $\lambda \neq \lambda_i\;\; \forall\; i=1..s$, then 
$\ve{A} - \lambda\ve{I}$ is invertible in each $W_{\lambda_i}$, and therefore there exist vectors 
$\ve{\zeta}_i = (\ve{A} - \lambda\ve{I})^{-1} \ve{u}_i \in W_{\lambda_i}$.
But then 
$\tilde{\ve{\zeta}} := \ve{\zeta} - \ve{\zeta}_1 - \dots - \ve{\zeta}_s$
is an eigenvalue of $\ve{A}$! 
This is impossible since $\lambda$ is not a root of
the characteristic polynomial, nor is $\tilde{\ve{\zeta}}=0$, since 
belonging to $\ve{\zeta}$ to $V^{\Complex}/W$ is not
a linear combination of elements in the $W_{\lambda_i}$.
We thus have a contradiction.
Now suppose that $\lambda = \lambda_j$ for some $j \in \{1..s\}$.
We can still define the vectors 
$\ve{\zeta}_i =(\ve{A} - \lambda_j\ve{I})^{-1} \ve{u}_i$ for all $i\neq j$, and
$\tilde{\ve{\zeta}}$, where we only subtract from $\ve{\zeta}$ all the 
$\ve{\zeta}_i$ with $i\neq j$,
therefore we have that 
\begin{equation}
  (\ve{A} - \lambda_j\ve{I})\tilde{\ve{\zeta}} = \ve{u}_i
\end{equation}
But applying $(\ve{A} - \lambda_j\ve{I})^{p_j}$ to this equation, with
$p_j$ the minimum value for which $W_{\lambda_j}{}_{p_j} = W_{\lambda_j}{}_{p_j+1}$, 
we get that $\tilde{\ve{\zeta}} \in W_{\lambda_j}$, and thus another contradiction,
therefore it can only be that $V^{\Complex}/W$ is the trivial space, and the $W_{\lambda_i}$
generate all of $V^{\Complex}$
\epru
\espa

We see that we only need to study each of these subspaces
$W_{\lambda_i}$ to find all their irreducible parts (from now on
we will suppress the subscript $i$).
But in these subspaces, the operator
$\ve{A}$ acts very simply!

Indeed, let $\ve{\Delta}_{\lambda}: W_{\lambda} \to W_{\lambda}$ 
be defined by  
$\ve{\Delta}_{\lambda}:= \ve{A}|_{W_{\lambda}} - \lambda \ve{I}|_{W_{\lambda}}$, 
then $\Delta$ has only $0$ as an eigenvalue, and therefore it is 
\textbf{nilpotent}~\index{nilpotente}, that is, there exists an integer $m \leq n$
such that $\ve{\Delta}_{\lambda}^m =0$.
\espa

\blem
Let $\ve{\Delta}: W \to W$ be such that its only eigenvalue is $0$, then 
$\ve{\Delta}$ is nilpotent.
\elem

\pru:
Let $W^p := \ve{\Delta}^p [W]$, then we have that $W^p \subseteq W^q$ if $p \geq q$.
Indeed, 
$W^p = \ve{\Delta}^p [W] = \ve{\Delta}^q[\ve{\Delta}^{p-q}[W]] \subset \ve{\Delta}^q[W]] $.
Since the dimension of $W$ is finite, it must happen that for some integer $p$, 
we will have that $W^p = W^{p+1}$, we see then that $\ve{\Delta}^p$ acts
injectively on $W^p$ and therefore cannot have $0$ as an eigenvalue.
But we have seen that every operator has some eigenvalue, and therefore we have
a contradiction unless $W^p = \{\ve{0}\}$. 
That is, $\ve{\Delta}^p = \ve{0}$
\epru
\espa

Nilpotent operators have the important property of generating a basis of the
space in which they act from their repeated application on a smaller set of linearly independent vectorsContinuing now with the study of invariant subspaces.
If $\det(\ve{A}-\lambda\ve{I})$ has $1\leq m \leq n$ distinct roots, 
$\{\lambda_i\}, \; i=1,\ldots,m$, then there will be at least one complex eigenvector 
$\ve{u}_i$ associated with each of them. 
Let's see that these form distinct invariant subspaces.

\espa
\blem
Let $\{(\lambda_i,\ve{u}_i)\}\;\;i=1\ldots m$ be a set of eigenvalue-eigenvector pairs.
If $\lambda_i\neq\lambda_j\;\;\;\;\forall\;i\neq j, \; i,j=1\ldots m$, then 
these eigenvectors are linearly independent.
\elem
\espa

\pru:
Suppose by contradiction that they are not, and therefore there exist constants 
$c^i\in\ve{C}$, $i=1,\ldots,m-1\,$, such that
\beq
\ve{u}_m=\sum_{i=1}^{m-1}\; c^i\,\ve{u}_i   \label{2_***}
\eeq
%
Applying $\ve{A}$ to both sides, we get
\beq
\ve{A}\ve{u}_m=\lambda_m\:\ve{u}_m=\sum_{i=1}^{m-1}\;c^i\,\lambda_i\,\ve{u}_i
\eeq
or,
\beq
0=\sum_{i=1}^{m-1}\;c^i\,(\lambda_m - \lambda_i)\,\ve{u}_i.
\eeq
We conclude that $\{\ve{u}_i\}\;\;i=1,\ldots,m-1$
are linearly dependent. Due to~\ref{2_***} and the hypothesis that the
eigenvalues are distinct, at least one of the coefficients must be non-zero,
and therefore we can solve for one of the remaining eigenvectors in terms of the others $m-2$. Repeating this procedure
$(m-1)$ times, we arrive at the conclusion that $\ve{u}_1=0$, which is a
contradiction since, as we have seen, the eigenvector equation always has
a non-trivial solution for each distinct eigenvalue\epru
\espa

If for each eigenvalue there is more than one eigenvector, then these form a higher-dimensional invariant vector subspace (reducible). Within each of these subspaces, we can take a basis composed of eigenvectors. The previous lemma ensures that all these eigenvectors thus chosen, for all eigenvalues, form a large linearly independent set.

\ejer: Convince yourself that the set of eigenvectors with the same eigenvalue forms
a vector subspace.

If a given operator $\ve{A}$ has all its eigenvalues distinct, then the corresponding eigenvectors are linearly independent and equal in number to the dimension of $V$, that is, they generate a basis of $V^{\Complex}$. 
In that basis, the matrix representation of $\ve{A}$ is diagonal, that is, $A^j{}_i = \delta^j{}_i \lambda_i$. 
Each of its eigenvectors generates an irreducible invariant subspace,
and together they generate $V^{\Complex}$. In each of them, the operator $\ve{A}$
acts merely by multiplication by $\lambda_i$. Note that the $\lambda_i$ are
generally complex, and therefore such multiplication is actually a rotation
plus a dilation. Note that, unlike the basis of Schur's triangulation lemma,
this is not generally orthogonal with respect to any given inner product.~\footnote{Note, however, that it can be \textsl{declared} orthogonal by defining
the inner product as 
$(\ve{u},\ve{v}) = \sum_{i=1}^{n} \ve{\theta}^i(\bar{\ve{u}})\ve{\theta}^i(\ve{v})$}
\espa

\ejem: Let $V$ be a vector space with $\dim(V)=2$ and let 
$\ve{A}$ be given by $\ve{A}(\ve{e}_1) = \ve{e}_2$, $\ve{A}(\ve{e}_2) = -\ve{e}_1$, where $(\ve{e}_1,\ve{e}_2)$ are any two linearly independent vectors. If we interpret them as two orthonormal vectors, then $A$ is a rotation by $\pi/2$ in the plane. 

Now let's calculate the determinant of $\ve{A} - \lambda \ve{I}$,

\begin{eqnarray}
\det(\ve{A}- \lambda\ve{I}) 
 & = &
\eps((\ve{A}- \lambda\ve{I})\ve{e}_1,(\ve{A}- \lambda\ve{I})\ve{e}_2)/
\eps(\ve{e}_1,\ve{e}_2) \nn
 & = &
\eps(\ve{e}_2-\lambda \ve{e}_1,-\ve{e}_1-\lambda \ve{e}_2)/
\eps(\ve{e}_1,\ve{e}_2) \nn
 & = &
 1 + \lambda^2.
\end{eqnarray}                        
%
and therefore the eigenvalues are $\lambda_{1} = \imath$, and $\lambda_2 = -\imath$.
The eigenvectors are $\ve{u}_1= \ve{e}_1+\imath \ve{e}_2$ and $\ve{u}_2=\ve{e}_1 - \imath \ve{e}_2 = \bar{\ve{u}}_1$. 
We see then that the action of $\ve{A}$ in these subspaces is multiplication by 
$\pm \imath$ and that both invariant subspaces are genuinely complex.
In this new basis, the space $V$ is generated by all linear combinations of
the form $z\ve{u}_1 + \bar{z}\ve{u}_2$, and the action of $\ve{A}$ is simply 
multiplication by $\imath$ of $z$.
\espa

If the multiplicity of any of the roots $\det(\ve{A}-\lambda\ve{I})=0$ is 
greater than one, there will be fewer eigenvalues than the dimension of the space, and therefore we will not be guaranteed to have enough eigenvectors to form a basis, as we can only guarantee the existence of one for each eigenvalue.

\ejem: Let $V$ be the set of 2-tuples of real numbers with a generic element
$(a,b)$ and let $\ve{A}$ be given by $\ve{A}(a,b) = (\lambda a+\epsilon b,\lambda b)$. Taking a basis, 
$\ve{e}_1=(1,0)$, 
$\ve{e}_2=(0,1)$, we see that its matrix representation is:

\beq\left(\barr{cc}
     \lambda & \epsilon    \\
     0 &  \lambda
     \earr\right)
\eeq
%
We see that this operator has only one eigenvalue with multiplicity 2. But it has only one eigenvector, proportional to $\ve{e}_1=(1,0)$. For future use, note that if we define 
$\ve{\Delta} = \ve{A}-\lambda \ve{I}$, then $\ve{e}_{1} = \frac{1}{\epsilon}\ve{\Delta}\ve{e}_2$,
and therefore, in the basis $\{\ve{\tilde{e}}_{1} = \ve{e}_{1}, \ve{\tilde{e}}_{2} = \frac{1}{\epsilon}\ve{e}_{2}\}$,
the operator is represented by the matrix,

\beq\left(\barr{cc}
     \lambda & 1    \\
     0 &  \lambda
     \earr\right)
\eeq

\espa

We must therefore analyze what happens in these cases. 
To do this, let us define, 
given $\lambda_i$ an eigenvalue of $\ve{A}$, the 
following subspaces:

\begin{equation}
  \label{eq:W_lm}
  W_{\lambda_i}{}_p = \{ \ve{u} \in V | \; (\ve{A} - \lambda_i \ve{I})^p \ve{u} = 0 \}
\end{equation}

Note that these are invariant spaces: 
$\ve{A} W_{\lambda_i}{}_p \subset W_{\lambda_i}{}_p$.
Moreover, $W_{\lambda_i}{}_p \subset W_{\lambda_i}{}_{p+1}$, and therefore, for a sufficiently large $p$ ($p \leq n$), we will have that 
$W_{\lambda_i}{}_p = W_{\lambda_i}{}_{p+1}$, taking the minimum among the
$p$'s where this occurs, we define $W_{\lambda_i} := W_{\lambda_i}{}_p$.
Note that if for some $\lambda_i$, $p=1$, then the subspace $W_{\lambda_i}$
is composed of eigenvectors.
These are the maximum invariant spaces associated with the eigenvalue $\lambda_i$,
indeed we have:

\blem
The only eigenvalue of $\ve{A}$ in $W_{\lambda_i}$ is $\lambda_i$.
\elem

\pru: Let $\lambda$ be an eigenvalue of $\ve{A}$ in $W_{\lambda_i}$. 
Let's see that $\lambda = \lambda_i$. 
As we have already seen, there will be an eigenvector $\ve{\zeta} \in W_{\lambda_i}$ 
with $\lambda$ as the eigenvalue.
Since it is in $W_{\lambda_i}$, there will be some $p \geq 1$ such that 
$(\ve{A} - \lambda_i \ve{I})^p \ve{\zeta} = 0$, but since it is an eigenvector, we have that
$(\lambda - \lambda_i)^p \ve{\zeta} = 0$, and therefore $\lambda = \lambda_i$
\epru
\espa

Now let's see that these subspaces are linearly independent and generate
all of $V^{\Complex}$. 
We will prove this theorem again in Chapter \ref{Sistemas_Lineales}.

\bteo[See Chapter \ref{Sistemas_Lineales}, Theorem \ref{5_teo_2}]
Given an operator $A: V \to V$, with eigenvectors $\{ \lambda_i \}, \; i=1...m$, 
the space $V^{\Complex}$ admits a direct decomposition into invariant subspaces 
$W_{\lambda_i}$,
where in each of them $\ve{A}$ has only $\lambda_i$ as an eigenvalue.
\eteo

\pru:

The $W_{\lambda_i}$ are independent. 
Let $\ve{v}_1 + \dots + \ve{v}_s = 0$, with
$\ve{v}_i \in W_{\lambda_i}$, then we must prove that each $\ve{v}_i=0$.
Applying 
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}$
to the previous sum, we get,
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}\ve{v}_1 =0$,
but since $\lambda_i$, $i \neq 1$, is not an eigenvalue of $\ve{A}$ in $W_{\lambda_1}$,
the operator 
$(\ve{A} - \lambda_2 \ve{I})^{p_2}\dots (\ve{A} - \lambda_s \ve{I})^{p_s}$ is 
invertible~\footnote{
Note that $(\ve{A} - \lambda_i \ve{I})^{s}|_{W_{\lambda_j}}$ 
is invertible if its determinant is non-zero. 
But 
$\det (\ve{A} - \lambda_i \ve{I})^{s} = (\det(\ve{A} - \lambda_i \ve{I}))^s 
= (\lambda_j - \lambda_i)^{s \dim (W_{\lambda_j})} \neq 0$
}
in that subspace, and therefore $\ve{v}_1 =0$. Continuing in this way, we see that all
the $\ve{v}_i$ must be zero.

The $W_{\lambda_i}$ generate all of $V^{\Complex}$. Suppose by contradiction that this
is not the case, and consider $V^{\Complex}/W$, where $W$ is the space generated by all
the $W_{\lambda_i}$, that is, the space of all linear combinations of 
elements in the $W_{\lambda_i}$. The operator $\ve{A}$ acts on $V^{\Complex}/W$
[$\ve{A}\{\ve{u}\} = \{\ve{A}\ve{u}\}$], and therefore it has an eigenvalue-eigenvector pair there. This implies that for some element $\ve{\zeta}$ of $V^{\Complex}$ 
in some equivalence class of $V^{\Complex}/W$, we have:
\begin{equation}
  \ve{A}\ve{\zeta} = \lambda \ve{\zeta} + \ve{u}_1 + \dots + \ve{u}_s
\end{equation}
%
where the $\ve{u}_i$ belong to each $W_{\lambda_i}$.
Now suppose that $\lambda \neq \lambda_i\;\; \forall\; i=1..s$, then 
$\ve{A} - \lambda\ve{I}$ is invertible in each $W_{\lambda_i}$, and therefore there exist vectors 
$\ve{\zeta}_i = (\ve{A} - \lambda\ve{I})^{-1} \ve{u}_i \in W_{\lambda_i}$.
But then 
$\tilde{\ve{\zeta}} := \ve{\zeta} - \ve{\zeta}_1 - \dots - \ve{\zeta}_s$
is an eigenvalue of $\ve{A}$! 
This is impossible since $\lambda$ is not a root of
the characteristic polynomial, nor is $\tilde{\ve{\zeta}}=0$, since 
belonging to $\ve{\zeta}$ to $V^{\Complex}/W$ is not
a linear combination of elements in the $W_{\lambda_i}$.
We thus have a contradiction.
Now suppose that $\lambda = \lambda_j$ for some $j \in \{1..s\}$.
We can still define the vectors 
$\ve{\zeta}_i =(\ve{A} - \lambda_j\ve{I})^{-1} \ve{u}_i$ for all $i\neq j$, and
$\tilde{\ve{\zeta}}$, where we only subtract from $\ve{\zeta}$ all the 
$\ve{\zeta}_i$ with $i\neq j$,
therefore we have that 
\begin{equation}
  (\ve{A} - \lambda_j\ve{I})\tilde{\ve{\zeta}} = \ve{u}_i
\end{equation}
But applying $(\ve{A} - \lambda_j\ve{I})^{p_j}$ to this equation, with
$p_j$ the minimum value for which $W_{\lambda_j}{}_{p_j} = W_{\lambda_j}{}_{p_j+1}$, 
we get that $\tilde{\ve{\zeta}} \in W_{\lambda_j}$, and thus another contradiction,
therefore it can only be that $V^{\Complex}/W$ is the trivial space, and the $W_{\lambda_i}$
generate all of $V^{\Complex}$
\epru
\espa

We see that we only need to study each of these subspaces
$W_{\lambda_i}$ to find all their irreducible parts (from now on
we will suppress the subscript $i$).
But in these subspaces, the operator
$\ve{A}$ acts very simply!

Indeed, let $\ve{\Delta}_{\lambda}: W_{\lambda} \to W_{\lambda}$ 
be defined by  
$\ve{\Delta}_{\lambda}:= \ve{A}|_{W_{\lambda}} - \lambda \ve{I}|_{W_{\lambda}}$, 
then $\Delta$ has only $0$ as an eigenvalue, and therefore it is 
\textbf{nilpotent}~\index{nilpotente}, that is, there exists an integer $m \leq n$
such that $\ve{\Delta}_{\lambda}^m =0$.
\espa

\blem
Let $\ve{\Delta}: W \to W$ be such that its only eigenvalue is $0$, then 
$\ve{\Delta}$ is nilpotent.
\elem

\pru:
Let $W^p := \ve{\Delta}^p [W]$, then we have that $W^p \subseteq W^q$ if $p \geq q$.
Indeed, 
$W^p = \ve{\Delta}^p [W] = \ve{\Delta}^q[\ve{\Delta}^{p-q}[W]] \subset \ve{\Delta}^q[W]] $.
Since the dimension of $W$ is finite, it must happen that for some integer $p$, 
we will have that $W^p = W^{p+1}$, we see then that $\ve{\Delta}^p$ acts
injectively on $W^p$ and therefore cannot have $0$ as an eigenvalue.
But we have seen that every operator has some eigenvalue, and therefore we have
a contradiction unless $W^p = \{\ve{0}\}$. 
That is, $\ve{\Delta}^p = \ve{0}$
\epru
\espa

Nilpotent operators have the important property of generating a basis of the space in which they act by repeatedly applying them to a smaller set of linearly independent vectors.

\blem
Let $\ve{\Delta}: W \to W$ be nilpotent, then there exists a basis of $W$ consisting of elements of the form:
\[
\{\{\ve{v}_1, \ve{\Delta}\ve{v}_1, \ldots ,\ve{\Delta}^{p_1}\ve{v}_1\},\ldots,
\{\{\ve{v}_d, \ve{\Delta}\ve{v}_d, \ldots ,\ve{\Delta}^{p_d}\ve{v}_d\}\}
\]
where $p_i$ is such that $\ve{\Delta}^{p_i+1}\ve{v}_i=0$
\elem
%
Note that if $n=\dim W$ then $n = \sum_{i=1}^{d} p_i$. 
Each of these sets formed by repeated applications of an operator is called a \textbf{cycle}~\index{ciclo}. In this case, the basis is formed by the elements of $d$ cycles. Note that cycles are not necessarily unique entities; indeed, if we have two cycles with the same number of elements, then any linear combination of them will also be a cycle.
Note that each cycle contains only one eigenvector.
\espa

\ejem: Consider the matrix,

\beq
\ve{\Delta}:=
     \left(\barr{cccc}
     0 &  1  &  0  &  0 \\
     0 &  0  &  1  &  0 \\
     0 &  0  &  0  &  1 \\
     0 &  0  &  0  &  0 
     \earr
     \right)
\eeq
%
its powers are,

\beq
\ve{\Delta}^{2}:=
     \left(\barr{cccc}
     0 &  0  &  1  &  0 \\
     0 &  0  &  0  &  1 \\
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 
     \earr
     \right) \;\;\;
     \ve{\Delta}^{3}:=
     \left(\barr{cccc}
     0 &  0  &  0  &  1 \\
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 
     \earr
     \right) \;\;\;
     \ve{\Delta}^{4}:=
     \left(\barr{cccc}
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 \\
     0 &  0  &  0  &  0 
     \earr
     \right) \;\;\;
\eeq
%
In this case, we have a single cycle, corresponding to the eigenvector $\ve{e}_{1}=(1,0,0,0)$, 
which is the vector whose span is $\ve{\Delta}^{3}[\re^{4}]$, the cycle is,
\begin{eqnarray*}
\ve{e_{4}}&=&(0,0,0,1) \\ 
\ve{e_{3}}&=&(0,0,1,0)=\ve{\Delta}\ve{e_{4}} \\
\ve{e_{2}}&=&(0,1,0,0)=\ve{\Delta}\ve{e_{3}}=\ve{\Delta}^{2}\ve{e_{4}} \\
\ve{e_{1}}&=&(1,0,0,0)=\ve{\Delta}\ve{e_{2}}=\ve{\Delta}^{2}\ve{e_{3}}=\ve{\Delta}^{3}\ve{e_{4}}.
\end{eqnarray*}

\ejem:
Consider the matrix,

\beq
\ve{\Delta}:=
     \left(\barr{cccccc}
     0 &  1  &  0  &  0 & 0 & 0\\
     0 &  0  &  0  &  0 & 0 & 0\\
     0 &  0  &  0  &  1 & 0 & 0 \\
     0 &  0  &  0  &  0 & 1 & 0 \\
     0 &  0  &  0  &  0 & 0 & 1 \\
     0 &  0  &  0  &  0 & 0 & 0 \\
     \earr
     \right)
\eeq
%
its non-trivial powers are,

\beq
\ve{\Delta}^{2}:=
     \left(\barr{cccccc}
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  1  &  0 \\
     0 &  0  &  0  &  0 &  0  &  1 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     \earr
     \right) \;\;\;
     \ve{\Delta}^{3}:=
     \left(\barr{cccccc}
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  1 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
     0 &  0  &  0  &  0 &  0  &  0 \\
          \earr
     \right) \nonumber
     \eeq
%
In this case, we have two cycles, corresponding to the two eigenvectors, $\ve{e_{1}}$ and $\ve{e_{3}}$.

\pru:
We will prove it by induction on the dimension of $W$. If $n=1$, we take any vector to generate the basis, since in this case $\ve{\Delta}=0$.
We now assume it is true for any dimension less than $n$.
In particular, since $\ve{\Delta}$ has a zero eigenvalue, 
$\dim(\ker \; \ve{\Delta}) \geq 1$, and therefore we have that $W' \;(= \ve{\Delta}(W))$
has a dimension less than $n$, say $n'$, and by the inductive hypothesis, a basis of the form
\[
\{\{\ve{v}'_1, \ve{\Delta}\ve{v}'_1, \ldots ,\ve{\Delta}^{p'_1}\ve{v}'_1\},\ldots,
\{\{\ve{v}'_{d'}, \ve{\Delta}\ve{v}'_{d'}, \ldots ,\ve{\Delta}^{p'_{d'}}\ve{v}'_{d'}\}\}
.
\]
% 
To form a basis of $W$, we will add to these vectors $d'$ vectors 
$\ve{v}_i$ such that $ \ve{\Delta}\ve{v}_i = \ve{v}'_i, \;\;i=1,\dots,d'$.
This can always be done since $\ve{v}'_i \in W' = \ve{\Delta}W$.
%
We thus see that we have increased the set of vectors to
\[
\{\{\ve{v}_1, \ve{\Delta}\ve{v}_1, \ldots ,\ve{\Delta}^{p'_1+1}\ve{v}_1\},\ldots,
\{\{\ve{v}_{d'}, \ve{\Delta}\ve{v}_{d'}, \ldots ,\ve{\Delta}^{p'_{d'}+1}\ve{v}_{d'}\}\},
\]
%
that is, we now have $r=\sum_{i=1}^{d'} (p'_i+1)= n' + d'$ vectors. 
%
To obtain a basis, we must then increase this set with $n-n'-d'$ vectors. 
Note that this number is non-negative; indeed, 
$n-n'= \dim(\ker\ve{\Delta}) \geq \dim(\ker\ve{\Delta} \cap W') = d'$, 
and it is precisely the dimension of the subspace of 
$\ker\ve{\Delta}$ that is not in $W'$.
We then complete the proposed basis for $W$ by incorporating into the already obtained set $n-n'-d'$ vectors $\{\ve{z}_i\}, i=1,..n-n'-d'$ from the null space of $\ve{\Delta}$
that are linearly independent among themselves and with the other elements of $\ker \ve{\Delta}$
in $W$ and that are also not in $W'$.
We have thus obtained a set of $d=d'+n-n'-d'=n-n'$ cycles.
Let's see that the set thus obtained is a basis. 
Since they are $n$ in number, we only need to see that they are linearly independent.
We must then prove that if we have constants $\{C_{i,j}\},\;\; i=1..d,\;j=0..p'_i+1$
such that 
\begin{equation}
  \ve{0} = \sum_{i=1}^d \sum_{j=0}^{p'_i+1} C_{ij}\ve{\Delta}^{p_i}\ve{v}_i
\end{equation}
then $C_{ij}=0$.
Applying $\ve{\Delta}$ to this relation, we get,
\begin{eqnarray}
  \ve{0} &=& \ve{\Delta}\sum_{i=1}^d \sum_{j=0}^{p'_i+1} C_{ij}\ve{\Delta}^{p_i}\ve{v}_i \nn 
         &=& \sum_{i=1}^{d'} \sum_{j=0}^{p'_i} C_{ij}\ve{\Delta}^{p'_i}\ve{v}'_i,
\end{eqnarray}
where we have used that $\ve{\Delta}^{p'_i+1}\ve{v}'_i=0$.
But this is the orthogonality relation of the basis of $W'$, and therefore
we conclude that $C_{ij}=0\;\forall i \leq d',\;\; j \leq p'_i$.
The initial relation is then reduced to 
\begin{eqnarray}
  \ve{0} &=& \sum_{i=1}^d  C_{ip'_i+1}\ve{\Delta}^{p'_i+1}\ve{v}_i \nn 
         &=& \sum_{i=1}^{d'} C_{ip'_i}\ve{\Delta}^{p'_i}\ve{v}'_i 
           + \sum_{i=d'+1}^{d} C_{i1}\ve{z}_i,
\end{eqnarray}
but the members of the first summation are part of the basis of $W'$ and therefore
linearly independent among themselves, while those of the second are a set
of elements outside $W'$ chosen to be linearly independent among themselves and with those of the
first summation, and therefore we conclude that all $C_{ij}$ are zero 
\epru
\espa
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Alternative Proof:}
%
Alternatively, the previous lemma can be proven constructively. 
Indeed, if $m+1$ is the power for which $\ve{\Delta}$ is nullified, 
we can take the space
$W^m = \ve{\Delta}^m[W]$ and a basis $\{\ve{v}^m_i\}$ of it. Note that all elements of 
$W^m$ are eigenvectors of $\ve{A}$, and therefore the elements of the basis.
Then we consider the space $W^{m-1} = \ve{\Delta}^{m-1}[W]$. 
Note that $W^{m} = \ve{\Delta}^m[W] = \ve{\Delta}^{m-1}[\ve{\Delta}[W]]$ and $\ve{\Delta}[W] \subset W$, 
therefore $W^{m} \subset W^{m-1}$.
Since $W^{m} = \ve{\Delta}[W^{m-1}]$,
for each vector $\ve{v}^{m}_i$ of the basis $\{\ve{v}^m_i\}$ of $W^m$ 
there will be a vector $\ve{v}^{m-1}_i$ such that $\ve{\Delta} \ve{v}^{m-1}_i = \ve{v}^m_i$.
Since $W^{m} \subset W^{m-1}$, the set $\{\ve{v}^m_i\} \cup \{\ve{v}^{m-1}_i\}$
is contained in $W^{m-1}$. 
Note that 
$\dim W^m = \dim (\ve{\Delta}[W^{m-1}]) \leq  \dim (\ker \ve{\Delta} \cap W^{m-1})$,
since $W^m \subset W^{m-1}$ and all its elements belong to $\ker \ve{\Delta} \cap W^{m-1}$.

Adding to the previous set a set $\{\ve{z}_i\}$ of 
{\small $\dim (\ker \ve{\Delta} \cap W^{m-1}) - \dim (\ker \ve{\Delta} \cap W^m)$} 
vectors from the null space of $\ve{\Delta}$ in $W^{m-1}$, such that they are linearly independent 
among themselves and with the elements of the basis of $W^m$, we obtain a set of 
$\dim W^{m-1}$ vectors.
Note that the mentioned choice of elements $\{\ve{z}_i\}$ can be made since
it is merely an extension of the basis $\{\ve{v}^m_i\}$ of $W^m$ to a basis of 
$\ker \ve{\Delta} \cap W^{m-1}$.
Now let's prove that they are linearly independent and therefore form a basis
of $W^{m-1}$. To do this, we need to prove that if
\begin{equation}
  \ve{0} = \sum_i C^m_i \ve{v}^m_i + \sum_i C^{m-1}_i \ve{v}^{m-1}_i + \sum_j C^z_j \ve{z}_j
\end{equation}
then each of the coefficients $C^m_i,\; C^{m-1}_i,\; C_j$ must be zero.
Multiplying the previous expression by $\ve{\Delta}$, we get,
\begin{equation}
  \ve{0} =  \sum_i C^{m-1}_i \ve{\Delta}\ve{v}^{m-1}_i = \sum_i C^{m-1}_i \ve{v}^{m}_i,
\end{equation}
but then the linear independence of the basis of $W^m$ ensures that the 
$\{C^{m-1}_i\}$ are all zero. We then have,
\begin{equation}
  \ve{0} = \sum_i C^m_i \ve{v}^m_i + \sum_j C^z_j \ve{z}_j.
\end{equation}
But these vectors were chosen to be linearly independent among themselves and therefore
all the coefficients in this sum must be zero. We thus see that the
set $\{\ve{v}^m_i\} \cup \{\ve{v}^{m-1}_i\} \cup \{\ve{z}_i\}$ forms a
cyclic basis of $W^{m-1}$. Continuing with $W^{m-2}$ and so on,
we obtain a cyclic basis for all of $W$
\epru

We thus see that the irreducible invariant subspaces of an operator are
constituted by cycles within invariant subspaces associated with a given
eigenvector. Each cycle contains a unique eigenvalue of the operator. 
We will denote the subspaces generated by these cycles (and usually also called
cycles) by $C^{k}_{\lambda_i}$, where the lower index refers to the eigenvalue of the
cycle and the upper index indexes the different cycles within each $W_{\lambda_i}$.

\ejer: Show that the obtained cycles are irreducible invariant subspaces of $\ve{A}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Jordan Canonical Form}
\label{sub:Forma_Canonica_de_Jordan}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\defi: {\sl Let $\ve{A}:V\to V$ be a linear operator.
We will say that $\ve{A}$ is of Jordan type
\index{Jordan!de tipo} with eigenvalue $\lambda$ if
there exists a basis $\{\ve{u}_i\}$ of $V$ such that~\footnote{
In the sense that $\ve{A}(\ve{v}) =\lambda \sum_{i=1}^n \; \ve{u}_i \ve{\theta}^i(\ve{v})
 + \sum_{i=2}^{n}\; \ve{u}_{i} \ve{\theta}^{i-1}(\ve{v}) \;\; \forall \ve{v} \in V$}

\beq
\ve{A} =\lambda \sum_{i=1}^n \; \ve{u}_i \ve{\theta}^i + \sum_{i=2}^{n}\;
\ve{u}_{i} \ve{\theta}^{i-1}\equiv \lambda I + \Delta
\eeq
\noi where $\{\ve{\theta}^i\}$ is the co-basis of the basis $\{\ve{u}_i\}$.

\rm
That is, in this basis, the components of $\ve{A}$ form a matrix
$A^j{}_i$ given by
\beq 
A=\left(\barr{ccccc}
          \lap &  1    &   0   &  \cdots    &   0    \\
            0  &\ddots &\ddots &  \ddots    &   \vdots  \\
               &       & \lap  &     1      &   0    \\
               &   0   &       &   \lap     &   1    \\
               &       &       &            & \lap 
            \earr\right)
\label{2.J1}
\eeq 
\noi Note that the matrix $\Delta $ is n-nilpotent, that is 
$\Delta^n = 0$.

Not every operator is of Jordan type ---find one that is not--- but it is clear that the restriction of any operator
to one of its irreducible invariant subspaces (cycles) is.
This can be seen by conveniently numbering the elements of the basis generated by the cycle.
\espa

\ejer: Find such an ordering of the elements of the basis.

Therefore, we can summarize the results found earlier in the
following theorem about the matrix representations of any operator
acting in a finite-dimensional space.

\bteo{de Jordan} Let $\ve{A}:V\to V$ be a linear operator
acting in a complex vector space $V$. Then, there exists a 
unique decomposition into a direct sum~\footnote{Recall that a vector space 
$V$ is said to be the direct sum of two vector spaces $W$ and $Z$,
and we denote it as $V = W \oplus Z$ if each vector in $V$ can be obtained in
a unique way as the sum of an element in $W$ and another in $Z$.} 
of $V$ into subspaces
$C^{k}_{\lambda_i}\;,\;\; V = C^1_{\lambda_1}\oplus \cdots C^{k_i}_{\lambda_1}\oplus \cdots
\oplus  C^1_{\lambda_d} \oplus \cdots C^{k_d}_{\lambda_d} \;,\;\;d\leq n $ such that 

i) The $C^k_{\lambda_i}$ are invariant under the action of $\ve{A}$, that is,
$\ve{A}\,C^k_{\lambda_i} \subseteq C^k_{\lambda_i}$

ii) The $C^k_{\lambda_i}$ are irreducible, that is, there are no 
invariant subspaces of $C^k_{\lambda_i}$ such that their sums are the entire $C^k_{\lambda_i}$.

iii) Due to property i), the operator $\ve{A}$ induces in each
$C^k_{\lambda_i}$ an operator $\ve{A}_i:C^k_{\lambda_i}\to C^k_{\lambda_i}$, 
which is of Jordan type
with $\lambda_i$ being one of the roots of the polynomial of degree $n_i$,
\beq
det(\ve{A_i}-\lambda_i \ve{I}) = 0.
\eeq
\eteo

This theorem tells us that given $\ve{A}$, there exists a basis, generally
complex, such that the matrix of its components has the form of square diagonal
blocks of $n_i \times n_i$, where $n_i$ is the
dimension of the subspace $C^k_{\lambda_i}$, each with the form given in~\ref{2.J1}. 
This form of the matrix is called the 
{\bf Jordan canonical form}.~\index{Jordan!forma canónica} 

\ejer: 
Show that the roots, $\lambda_i$, that appear
in the operators $\ve{A}_i$ are invariant under similarity transformations, that is, $\lambda_i(\ve{A})=\lambda_i(\ve{P}\ve{A}\ve{P}^{-1})$.
\espa
\ejem: 
Let $\ve{A}:\ve{C}^3\to \ve{C}^3$, then $det(\ve{A}-\lambda\ve{I})$
is a polynomial of degree 3, and therefore has three roots. If these are distinct, there will be at least three
invariant and irreducible subspaces of $\ve{C}^3$, but
$dim\ve{C}^3=3$ and therefore each of them has $n_i=1$. The
Jordan canonical form is then,

\noi If two of them coincide, we have two possibilities: either we have
three subspaces, in which case the Jordan canonical form will be 
\beq\left(\barr{ccc}
     \lap_1 & 0  &  0  \\
     0 &  \lap  &  0  \\
     0  &  0  &  \lap
     \earr\right)
\eeq
\noi or, we have two subspaces, one necessarily of dimension 2,
the Jordan canonical form will be 
\beq \left(\barr{ccc}
     \lap_1 & 0  &  0  \\
     0 &  \lap  &  1  \\
     0  &  0  &  \lap
     \earr\right)
\eeq
If all three roots coincide, then there will be three possibilities,
\beq\barr{ccc}
      \left(\barr{ccc}
     \lap & 0  &  0  \\
     0 &  \lap  &  0  \\
     0  &  0  &  \lap
     \earr\right),  &  \left(\barr{ccc}
                        \lap & 0  &  0  \\
                         0 &  \lap  &  1  \\
                         0  &  0  &  \lap
                            \earr\right)\;and\;\;  & \left(\barr{ccc}
                                                    \lap & 1  &  0  \\
                                                       0 &  \lap  &  1  \\
                                                         0  &  0  &  \lap
                                                           \earr\right)
\earr
\eeq
\espa

\ejem:
We will now illustrate the case of coincident eigenvalues in two
dimensions. This case is not generic, in the sense that any
perturbation in the system ---that is, any minimal change in
the equations--- separates the roots, making them distinct. 

Let $\ve{A}:\ve{C}^2\to\ve{C}^2$. In this case, the characteristic polynomial
$det(\ve{A}-\lambda\ve{I})$ has only two roots, which we will assume are coincident, $\lambda_1=\lambda_2=\lambda$. We
find ourselves with two possibilities: either there are two linearly independent eigenvectors
$\ve{u}_1$ and $\ve{u}_2$, in which case $V=B_1\oplus
B_2$ and $\ve{A}$ is diagonalizable
($A=diag(\lambda,\lambda)=\ve{I}\lambda)$, 
or there is only one eigenvector
$\tilde{\ve{u}}_1$. In this case, let $\tilde{\ve{u}}_2$ be any vector
linearly independent of $\tilde{\ve{u}}_1$, then
$\ve{A}\tilde{\ve{u}}_2=c^1\tilde{\ve{u}}_1+c^2\tilde{\ve{u}}_2 $
for some scalars
$c^1$ and $c^2$ in $\ve{C}$. Calculating the determinant of
$\ve{A}-\tilde{\lambda}\ve{I}$ in this basis, we get
$(\lambda-\tilde{\lambda})(c^2-\tilde{\lambda})$, but $\lambda$ is a
double root and therefore $c^2=\lambda$.

Reordering and rescaling the bases 
$\ve{u}_1=\tilde{\ve{u}}_1 \;,\;\; \ve{u}_2=c^1\tilde{\ve{u}}_2$, we obtain

\beq\barr{rcl}
      \ve{A}\,\ve{u}_1 & = & \lambda\,\ve{u}_1   \\
      \ve{A}\,\ve{u}_2 & = & \lambda \ve{u}_2 + \ve{u}_1 , 
      \earr
\eeq
and therefore 
\beq
\ve{A}=\lambda (\ve{u}_1\oplus\ve{\theta}^1+\ve{u}_2\oplus\ve{\theta}^2)
+\ve{u}_1\oplus\ve{\theta}^2,
\eeq
\noi where $\{\ve{\theta}^i\}$ is the co-basis of the basis $\{\ve{u}_i\}$.

Note that $(\ve{A} -\lambda\ve{I})\,\ve{u}_2=\ve{u}_1$ and
$(\ve{A}-\lambda\ve{I})\ve{u}_1=0 $, that is, $\Delta^2 = (\ve{A}-\lambda
\ve{I})^2 =0$. 

As we will see later in physical applications, the
invariant subspaces have a clear physical meaning,
they are called normal modes ---one-dimensional case--- and 
cycles ---in other cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Similarity Relation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In physics applications, the following equivalence relation is common: [See box at the end of the chapter.] 
We will say that the operator $\ve{A}$ is similar to
the operator $\ve{B}$ if there exists another operator $\ve{P}$, invertible, such
that 
\beq
\ve{A}=\ve{P}\ve{B}\ve{P}^{-1}.
\eeq
That is, if we {\it rotate} $V$ with an invertible operator
$\ve{P}$ and then apply $\ve{A}$, we obtain the same action as
if we first apply $\ve{B}$ and then {\it rotate} with $\ve{P}$.

\espa
\ejer:

\noi a) Prove that similarity is an equivalence relation.

\noi b) Prove that the functions and maps defined above
are the same in the different equivalence classes, that is,
\beq\barr{rcl}
  det(\ve{P}\ve{A}\ve{P}^{-1}) & = & det(\ve{A})\\
  tr(\ve{P}\ve{A}\ve{P}^{-1})  & = & tr(\ve{A}) \\
  e^{\ve{P}\ve{A}\ve{P}^{-1}}  & = & \ve{P} e^{\ve{A}}\ve{P}^{-1}.
  \earr
\eeq


\vfill
\newpage


\section{Adjoint Operators}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\ve{A}$ be a linear operator between two vector spaces, 
$\ve{A}: V \to W$, that is,
$\ve{A} \in {\cal{L}}(V,W)$.
Since $V$ and $W$ have dual spaces, this operator naturally induces a linear operator from $W'$ to $V'$, called its 
\textbf{dual}~\index{operador!dual},
\begin{equation}
  \label{eq:dual}
  \ve{A}'(\ve{\omega})(\ve{v}) := \ve{\omega}(\ve{A}(\ve{v})) \;\;\; 
                                   \forall \;\; \ve{\omega} \in W', \;\;\;
                                   \ve{v} \in V.
\end{equation}
%
That is, the operator that when applied to an element $\ve{\omega} \in W'$
gives us the element $\ve{A}'(\ve{\omega})$ of $V'$ which, when acting
on $\ve{v} \in V$, gives the number $\ve{\omega}(\ve{A}(\ve{v}))$.
See figure.

\espa 


\begin{figure}[htbp]
  \begin{center}
    \resizebox{5cm}{!}{\myinput{Figure/m2_2}}
    \caption{Diagram of the dual operator.}
    \label{fig:2_2}
  \end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Note that this is a linear operator since,
\begin{eqnarray}
  \label{eq:dual_linealidad}
  \ve{A}'(\alpha \ve{\omega} + \ve{\sigma})(\ve{v}) &=& 
                         (\alpha \ve{\omega} + \ve{\sigma})(\ve{A}(\ve{v})) \nn
                                                    &=&
             \alpha \ve{\omega}(\ve{A}(\ve{v})) + \ve{\sigma}(\ve{A}(\ve{v})) \nn
                                                    &=&
             \alpha \ve{A}'(\ve{\omega})(\ve{v}) + \ve{A}'(\ve{\sigma})(\ve{v}).
\end{eqnarray}
%

In the matrix representation, this operator is represented merely by the same matrix as the original, but now acting on the left, ${A}'({w})_i = {w}_j A^j{}_i$, that is, $A'_i{}^j = A^j{}_i$.


If there are norms defined in $V$ and $W$ and we define the norm of 
$\ve{A}: V \to W$ in the usual way,

\begin{equation}
  \label{eq:norma_A}
  \|\ve{A}\| := \sup_{\|\ve{v}\|_V=1}\{\|\ve{A}(\ve{v})\|_W\}
\end{equation}
%
Then we see that 

\begin{eqnarray}
  \label{eq:norma_A_dual}
  \|\ve{A}'\| &:=& 
            \sup_{\|\ve{\omega}\|_{W'}=1}\{\|\ve{A}'(\ve{\omega})\|_{V'}\} \nn
              &=& 
            \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{|\ve{A}'(\ve{\omega})(\ve{v})|\}\} \nn
              &=&
              \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{|\ve{\omega}(\ve{A}(\ve{v}))|\}\} \nn
              &\leq&
              \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{\|\ve{\omega}\|_{W'}
                           \|(\ve{A}(\ve{v}))\|_{W}\}\} \nn
              &=&
           \sup_{\|\ve{v}\|_V=1}\{\|(\ve{A}(\ve{v}))\|_{W}\} \nn
              &=& \|\ve{A}\|.
\end{eqnarray}
%
Thus we see that if an operator is bounded, then its dual is also bounded.
In fact, the equality of the norms can be proven, but this would require introducing new tools (in the most general case, the Hahn-Banach theorem) that we do not wish to incorporate in this text.

Let's see what the components of the dual of an operator are in terms of
the components of the original operator. 
Let $\{\ve{e}_i\}$, $\{\ve{\theta}^i\}$, $i=1,..n$ be a basis and respectively
a co-basis of $V$, and let $\{\hat{\ve{e}}_i\}$, $\{\hat{\ve{\theta}}^i\}$, 
$i=1,..m$
be a pair of basis and co-basis of $W$.
We then have that the components of $\ve{A}$ with respect to these bases
are: 
$A^i{}_j := \hat{\ve{\theta}}^i(\ve{A}(\ve{e}_j))$, 
that is,
$\ve{A}(\ve{v}) = 
\sum_{i=1}^{m} \sum_{j=1}^{n} A^i{}_j \hat{\ve{e}}_i\ve{\theta}^j(\ve{v})$.

Therefore, if $\ve{v}$ has components $(v^1,v^2,\dots,v^n)$ in the 
basis $\{\ve{e}_i\}$, $\ve{A}(\ve{v})$ has components 
\[
(\sum_{i=1}^{n} A^1{}_iv^i, \sum_{i=1}^{n} A^2{}_iv^i,\dots,\sum_{i=1}^{n} A^m{}_iv^i)
\]
%
 in the basis $\{\hat{\ve{e}}_i\}$

Now let's see the components of $\ve{A}'$. 
By definition we have,

\[
\ve{A}'{}^i{}_j := \ve{A}'(\hat{\ve{\theta}}^i)(\ve{e}_j) 
                  =  \hat{\ve{\theta}}^i(\ve{A}(\ve{e}_j)) = A^i{}_j.
\]
That is, the same components, but now the matrix acts on the left on
the components $(\omega_1,\omega_2,\dots, \omega_m)$ of an element 
$\ve{\omega}$ of $W'$ in the co-basis $\{\hat{\ve{\theta}}^i\}$.
The components of $\ve{A}'(\omega)$ in the co-basis $\{\ve{\theta}^i\}$ are,
\[ 
(\sum_{i=1}^{m}A^i{}_1\omega_i, \sum_{i=1}^{m}A^i{}_2\omega_i,\dots,\sum_{i=1}^{m}A^i{}_n\omega_i).
\]
%
A particularly interesting case of this construction is when
$W=V$ and this is a space with an inner product, that is, a 
Hilbert space. In this case, the inner product gives us a canonical map between
$V$ and its dual $V'$:
\begin{equation}
  \label{eq:mapa_canonico}
  \phi:V \to V', \;\;\;\; \phi(\ve{v}) := \langle \ve{v},\cdot \rangle.
\end{equation}
%
This map is injective, and since $V$ and $V'$ have the same dimension, it is also surjective, and therefore invertible. 
That is, given $\omega \in V'$, there exists $\ve{v}=\phi^{-1}(\omega) \in V$ such that $\langle \ve{v},\cdot \rangle = \omega$. 
Note then that $\phi^{-1}: V' \to V$ satisfies, 
\[
\langle \phi^{-1}(\ve{\omega}),\ve{u} \rangle = \langle \ve{v},\ve{u} \rangle = \ve{\omega}(\ve{u}).
\]


If $\ve{A}: V \to V$, then $\ve{A}': V' \to V'$
can also be considered as an operator between $V$ and $V$
which we will call $\ve{A}^{\star}$.


%\[
%((\phi^{-1}(\phi(\ve{v})),\ve{u}) = ((\phi^{-1}((\ve{v},\cdot)),\ve{u}) 
%= (\ve{v},\ve{u}) \;\;\;\; \forall \ve{u}\; \in V.
%\]
%
With the help of this map, we define $\ve{A}^{\star}: V \to V$ given by:
(See figure)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\espa 


\begin{figure}[htbp]
  \begin{center}
    \resizebox{5cm}{!}{\myinput{Figure/m2_3}}
    \caption{Diagram of the star operator.}
    \label{fig:2_3}
  \end{center}
\end{figure}

\begin{equation}
  \label{eq:A_adjunto}
  \ve{A}^{\star}(\ve{v}) := \phi^{-1}(\ve{A}'(\phi(\ve{v}))).
\end{equation}
%
In terms of the inner product, this is:
\begin{equation}
  \label{eq:A_adjunto_2}
  \langle \ve{A}^{\star}(\ve{v}),\ve{u} \rangle = \langle \phi^{-1}(\ve{A}'(\phi(\ve{v}))),\ve{u} \rangle
                                               = \ve{A}'(\phi(\ve{v}))(\ve{u}) 
                                               = \phi(\ve{v})(\ve{A}(\ve{u}))
                                               = \langle \ve{v},\ve{A}(\ve{u}) \rangle.
\end{equation}
%
In its matrix representation, this operator is, 
\begin{eqnarray}
A^{\star}{}^j{}_i &=& t_{il} A^l{}_k (t^{-1}){}^{kj} , \;\;\;\; \mbox{real inner product} \\
A^{\star}{}^j{}_i &=& t_{il} \bar{A}^l{}_k (t^{-1}){}^{kj} , \;\;\;\; \mbox{complex inner product} 
\end{eqnarray}
%
where $t_{li}$ is the representation of the inner product and $(t^{-1}){}^{jk}$ that of its inverse 
($t_{il} (t^{-1})^{lk} = \delta^k{}_i$). 
If we choose a basis such that $t_{ik} = \delta_{ik}$, the Kronecker delta, the matrix representation of the adjoint is simply the transpose matrix, $A^{\star}{}^j{}_i = A^{\dagger}{}^j{}_i = A^i{}_j$, 

A particularly interesting subset of operators is those for which 
$\ve{A} = \ve{A}^{\star}$. These operators are called 
\textbf{Hermitian}~\index{operadores!Hermitianos} or 
\textbf{Self-adjoint}~\index{operadores!Autoadjuntos}.~\footnote{
In the case of infinite dimension, these names do not coincide for
some authors.}

Self-adjoint operators have important properties:

\blem
Let $M = Span\{\ve{u}_{1},\ve{u}_2,\dots,\ve{u}_m\}$, where $\{\ve{u}_i\}$
are a set of eigenvalues of $\ve{A}$, a self-adjoint operator.
Then $M$ and $M^{\perp}$ are invariant spaces of $\ve{A}$.
\elem

\bpru 
The first statement is clear and general, the second depends on the
Hermiticity of $\ve{A}$. Let $\ve{v} \in M^{\perp}$ be any vector, let's see
that $\ve{A}(\ve{v}) \in M^{\perp}$. Let $\ve{u} \in M$ be arbitrary, then
\begin{equation}
  \langle \ve{u},\ve{A}(\ve{v}) \rangle = \langle \ve{A}(\ve{u}),\ve{v} \rangle = 0,
\end{equation}
%
since $\ve{A}(\ve{u}) \in M$ if $\ve{u} \in M$
\epru
\espa
This property has the following corollary:

\bcor
Let $\ve{A}:H \to H$ be self-adjoint. Then the eigenvectors of $\ve{A}$ form an orthonormal basis of $H$.
\ecor

\bpru
$\ve{A}: H \to H$ has at least one eigenvector, let's call it $\ve{u}_1$. 
Now consider its restriction to the space perpendicular to $\ve{u}_1$
which we also denote by $\ve{A}$ since by the previous lemma this is
an invariant space, $\ve{A}:\{\ve{u}_1\}^{\perp} \to \{\ve{u}_1\}^{\perp}$.
This operator also has an eigenvector, say $\ve{u}_2$ and 
$\ve{u}_1 \perp \ve{u}_2$. Now consider the restriction of $\ve{A}$ to
$Span\{\ve{u}_1,\ve{u}_2\}^{\perp}$, there we also have 
$\ve{A}:Span\{\ve{u}_1,\ve{u}_2\}^{\perp} \to Span\{\ve{u}_1,\ve{u}_2\}^{\perp}$
and therefore an eigenvector of $\ve{A}$, $\ve{u}_3$ with 
$\ve{u}_3 \perp \ve{u}_1$, $\ve{u}_3 \perp \ve{u}_2$.
Continuing in this way, we end up with $n = \dim H$ eigenvectors all
orthogonal to each other
\epru

This theorem has several extensions to the case where the vector space is
of infinite dimension. 
Later in chapter \ref{ecuaciones_elipticas} we will see one of them.

Note that in this basis $\ve{A}$ is diagonal and therefore we have
\bcor
Every self-adjoint operator is diagonalizable
\ecor
%
Also note that if $\ve{u}$ is an eigenvector of $\ve{A}$ self-adjoint,
with eigenvalue $\lambda$, then,
\begin{equation} 
  \bar{\lambda} \langle \ve{u},\ve{u} \rangle = \langle \ve{A}(\ve{u}),\ve{u} \rangle
                               = \langle \ve{u},\ve{A}(\ve{u})\rangle
                               = \lambda \langle \ve{u},\ve{u} \rangle
\end{equation}
and therefore $\bar{\lambda} = \lambda$, that is,
    
\blem
The eigenvalues of a self-adjoint operator are real
\elem

Let's see what the condition of Hermiticity means in terms
of the components of the operator in an orthonormal basis.
Let $\ve{A}$ be a self-adjoint operator and let 
$\{\ve{e}_i\}$, $i=1,\dots,n$
be an orthonormal basis of the space where it acts.
We have that $\langle \ve{A}(\ve{e}_i),\ve{e}_j \rangle = \langle \ve{e}_i,\ve{A}(\ve{e}_j) \rangle$
and therefore, noting that 
$\ve{I} = \sum_{i=1}^{n} \ve{e}_i\ve{\theta}^i$,
we obtain,
\begin{eqnarray}
  0 &=& \langle \sum_{k=1}^{n} \ve{e}_k\ve{\theta}^k(\ve{A}(\ve{e}_i)),\ve{e}_j \rangle
       - \langle \ve{e}_i,\sum_{l=1}^{n} \ve{e}_l\ve{\theta}^l(\ve{A}(\ve{e}_j)) \rangle \nn
    &=& \sum_{k=1}^{n} \bar{A}^k{}_i \langle \ve{e}_k,\ve{e}_j \rangle
       -\sum_{l=1}^{n} A^l{}_j \langle \ve{e}_i,\ve{e}_l \rangle \nn
    &=& \sum_{k=1}^{n} \bar{A}^k{}_i \delta_{kj} 
       - \sum_{l=1}^{n} A^l{}_j \delta_{li} \nn
    &=&  \bar{A}^j{}_i - A^i{}_j
\end{eqnarray}
%
from which we conclude that 

\begin{equation}
  \bar{A}^j{}_i = A^i{}_j
\end{equation}
%
that is, the transpose matrix is the complex conjugate of the original.
In the case of real matrices, we see that the condition is that in this basis
the matrix is equal%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Adjoint Operators}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\ve{A}$ be a linear operator between two vector spaces, 
$\ve{A}: V \to W$, that is,
$\ve{A} \in {\cal{L}}(V,W)$.
Since $V$ and $W$ have dual spaces, this operator naturally induces a linear operator from $W'$ to $V'$, called its 
\textbf{dual}~\index{operador!dual},
\begin{equation}
  \label{eq:dual}
  \ve{A}'(\ve{\omega})(\ve{v}) := \ve{\omega}(\ve{A}(\ve{v})) \;\;\; 
                                   \forall \;\; \ve{\omega} \in W', \;\;\;
                                   \ve{v} \in V.
\end{equation}
%
That is, the operator that when applied to an element $\ve{\omega} \in W'$
gives us the element $\ve{A}'(\ve{\omega})$ of $V'$ which, when acting
on $\ve{v} \in V$, gives the number $\ve{\omega}(\ve{A}(\ve{v}))$.
See figure.

\espa 


\begin{figure}[htbp]
  \begin{center}
    \resizebox{5cm}{!}{\myinput{Figure/m2_2}}
    \caption{Diagram of the dual operator.}
    \label{fig:2_2}
  \end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Note that this is a linear operator since,
\begin{eqnarray}
  \label{eq:dual_linealidad}
  \ve{A}'(\alpha \ve{\omega} + \ve{\sigma})(\ve{v}) &=& 
                         (\alpha \ve{\omega} + \ve{\sigma})(\ve{A}(\ve{v})) \nn
                                                    &=&
             \alpha \ve{\omega}(\ve{A}(\ve{v})) + \ve{\sigma}(\ve{A}(\ve{v})) \nn
                                                    &=&
             \alpha \ve{A}'(\ve{\omega})(\ve{v}) + \ve{A}'(\ve{\sigma})(\ve{v}).
\end{eqnarray}
%

In the matrix representation, this operator is represented merely by the same matrix as the original, but now acting on the left, ${A}'({w})_i = {w}_j A^j{}_i$, that is, $A'_i{}^j = A^j{}_i$.


If there are norms defined in $V$ and $W$ and we define the norm of 
$\ve{A}: V \to W$ in the usual way,

\begin{equation}
  \label{eq:norma_A}
  \|\ve{A}\| := \sup_{\|\ve{v}\|_V=1}\{\|\ve{A}(\ve{v})\|_W\}
\end{equation}
%
Then we see that 

\begin{eqnarray}
  \label{eq:norma_A_dual}
  \|\ve{A}'\| &:=& 
            \sup_{\|\ve{\omega}\|_{W'}=1}\{\|\ve{A}'(\ve{\omega})\|_{V'}\} \nn
              &=& 
            \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{|\ve{A}'(\ve{\omega})(\ve{v})|\}\} \nn
              &=&
              \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{|\ve{\omega}(\ve{A}(\ve{v}))|\}\} \nn
              &\leq&
              \sup_{\|\ve{\omega}\|_{W'}=1}\{ 
            \sup_{\|\ve{v}\|_V=1}\{\|\ve{\omega}\|_{W'}
                           \|(\ve{A}(\ve{v}))\|_{W}\}\} \nn
              &=&
           \sup_{\|\ve{v}\|_V=1}\{\|(\ve{A}(\ve{v}))\|_{W}\} \nn
              &=& \|\ve{A}\|.
\end{eqnarray}
%
Thus we see that if an operator is bounded, then its dual is also bounded.
In fact, the equality of the norms can be proven, but this would require introducing new tools (in the most general case, the Hahn-Banach theorem) that we do not wish to incorporate in this text.

Let's see what the components of the dual of an operator are in terms of
the components of the original operator. 
Let $\{\ve{e}_i\}$, $\{\ve{\theta}^i\}$, $i=1,..n$ be a basis and respectively
a co-basis of $V$, and let $\{\hat{\ve{e}}_i\}$, $\{\hat{\ve{\theta}}^i\}$, 
$i=1,..m$
be a pair of basis and co-basis of $W$.
We then have that the components of $\ve{A}$ with respect to these bases
are: 
$A^i{}_j := \hat{\ve{\theta}}^i(\ve{A}(\ve{e}_j))$, 
that is,
$\ve{A}(\ve{v}) = 
\sum_{i=1}^{m} \sum_{j=1}^{n} A^i{}_j \hat{\ve{e}}_i\ve{\theta}^j(\ve{v})$.

Therefore, if $\ve{v}$ has components $(v^1,v^2,\dots,v^n)$ in the 
basis $\{\ve{e}_i\}$, $\ve{A}(\ve{v})$ has components 
\[
(\sum_{i=1}^{n} A^1{}_iv^i, \sum_{i=1}^{n} A^2{}_iv^i,\dots,\sum_{i=1}^{n} A^m{}_iv^i)
\]
%
 in the basis $\{\hat{\ve{e}}_i\}$

Now let's see the components of $\ve{A}'$. 
By definition we have,

\[
\ve{A}'{}^i{}_j := \ve{A}'(\hat{\ve{\theta}}^i)(\ve{e}_j) 
                  =  \hat{\ve{\theta}}^i(\ve{A}(\ve{e}_j)) = A^i{}_j.
\]
That is, the same components, but now the matrix acts on the left on
the components $(\omega_1,\omega_2,\dots, \omega_m)$ of an element 
$\ve{\omega}$ of $W'$ in the co-basis $\{\hat{\ve{\theta}}^i\}$.
The components of $\ve{A}'(\omega)$ in the co-basis $\{\ve{\theta}^i\}$ are,
\[ 
(\sum_{i=1}^{m}A^i{}_1\omega_i, \sum_{i=1}^{m}A^i{}_2\omega_i,\dots,\sum_{i=1}^{m}A^i{}_n\omega_i).
\]
%
A particularly interesting case of this construction is when
$W=V$ and this is a space with an inner product, that is, a 
Hilbert space. In this case, the inner product gives us a canonical map between
$V$ and its dual $V'$:
\begin{equation}
  \label{eq:mapa_canonico}
  \phi:V \to V', \;\;\;\; \phi(\ve{v}) := \langle \ve{v},\cdot \rangle.
\end{equation}
%
This map is injective, and since $V$ and $V'$ have the same dimension, it is also surjective, and therefore invertible. 
That is, given $\omega \in V'$, there exists $\ve{v}=\phi^{-1}(\omega) \in V$ such that $\langle \ve{v},\cdot \rangle = \omega$. 
Note then that $\phi^{-1}: V' \to V$ satisfies, 
\[
\langle \phi^{-1}(\ve{\omega}),\ve{u} \rangle = \langle \ve{v},\ve{u} \rangle = \ve{\omega}(\ve{u}).
\]


If $\ve{A}: V \to V$, then $\ve{A}': V' \to V'$
can also be considered as an operator between $V$ and $V$
which we will call $\ve{A}^{\star}$.


%\[
%((\phi^{-1}(\phi(\ve{v})),\ve{u}) = ((\phi^{-1}((\ve{v},\cdot)),\ve{u}) 
%= (\ve{v},\ve{u}) \;\;\;\; \forall \ve{u}\; \in V.
%\]
%
With the help of this map, we define $\ve{A}^{\star}: V \to V$ given by:
(See figure)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\espa 


\begin{figure}[htbp]
  \begin{center}
    \resizebox{5cm}{!}{\myinput{Figure/m2_3}}
    \caption{Diagram of the star operator.}
    \label{fig:2_3}
  \end{center}
\end{figure}

\begin{equation}
  \label{eq:A_adjunto}
  \ve{A}^{\star}(\ve{v}) := \phi^{-1}(\ve{A}'(\phi(\ve{v}))).
\end{equation}
%
In terms of the inner product, this is:
\begin{equation}
  \label{eq:A_adjunto_2}
  \langle \ve{A}^{\star}(\ve{v}),\ve{u} \rangle = \langle \phi^{-1}(\ve{A}'(\phi(\ve{v}))),\ve{u} \rangle
                                               = \ve{A}'(\phi(\ve{v}))(\ve{u}) 
                                               = \phi(\ve{v})(\ve{A}(\ve{u}))
                                               = \langle \ve{v},\ve{A}(\ve{u}) \rangle.
\end{equation}
%
In its matrix representation, this operator is, 
\begin{eqnarray}
A^{\star}{}^j{}_i &=& t_{il} A^l{}_k (t^{-1}){}^{kj} , \;\;\;\; \mbox{real inner product} \\
A^{\star}{}^j{}_i &=& t_{il} \bar{A}^l{}_k (t^{-1}){}^{kj} , \;\;\;\; \mbox{complex inner product} 
\end{eqnarray}
%
where $t_{li}$ is the representation of the inner product and $(t^{-1}){}^{jk}$ that of its inverse 
($t_{il} (t^{-1})^{lk} = \delta^k{}_i$). 
If we choose a basis such that $t_{ik} = \delta_{ik}$, the Kronecker delta, the matrix representation of the adjoint is simply the transpose matrix, $A^{\star}{}^j{}_i = A^{\dagger}{}^j{}_i = A^i{}_j$, 

A particularly interesting subset of operators is those for which 
$\ve{A} = \ve{A}^{\star}$. These operators are called 
\textbf{Hermitian}~\index{operadores!Hermitianos} or 
\textbf{Self-adjoint}~\index{operadores!Autoadjuntos}.~\footnote{
In the case of infinite dimension, these names do not coincide for
some authors.}

Self-adjoint operators have important properties:

\blem
Let $M = Span\{\ve{u}_{1},\ve{u}_2,\dots,\ve{u}_m\}$, where $\{\ve{u}_i\}$
are a set of eigenvalues of $\ve{A}$, a self-adjoint operator.
Then $M$ and $M^{\perp}$ are invariant spaces of $\ve{A}$.
\elem

\bpru 
The first statement is clear and general, the second depends on the
Hermiticity of $\ve{A}$. Let $\ve{v} \in M^{\perp}$ be any vector, let's see
that $\ve{A}(\ve{v}) \in M^{\perp}$. Let $\ve{u} \in M$ be arbitrary, then
\begin{equation}
  \langle \ve{u},\ve{A}(\ve{v}) \rangle = \langle \ve{A}(\ve{u}),\ve{v} \rangle = 0,
\end{equation}
%
since $\ve{A}(\ve{u}) \in M$ if $\ve{u} \in M$
\epru
\espa
This property has the following corollary:

\bcor
Let $\ve{A}:H \to H$ be self-adjoint. Then the eigenvectors of $\ve{A}$ form an orthonormal basis of $H$.
\ecor

\bpru
$\ve{A}: H \to H$ has at least one eigenvector, let's call it $\ve{u}_1$. 
Now consider its restriction to the space perpendicular to $\ve{u}_1$
which we also denote by $\ve{A}$ since by the previous lemma this is
an invariant space, $\ve{A}:\{\ve{u}_1\}^{\perp} \to \{\ve{u}_1\}^{\perp}$.
This operator also has an eigenvector, say $\ve{u}_2$ and 
$\ve{u}_1 \perp \ve{u}_2$. Now consider the restriction of $\ve{A}$ to
$Span\{\ve{u}_1,\ve{u}_2\}^{\perp}$, there we also have 
$\ve{A}:Span\{\ve{u}_1,\ve{u}_2\}^{\perp} \to Span\{\ve{u}_1,\ve{u}_2\}^{\perp}$
and therefore an eigenvector of $\ve{A}$, $\ve{u}_3$ with 
$\ve{u}_3 \perp \ve{u}_1$, $\ve{u}_3 \perp \ve{u}_2$.
Continuing in this way, we end up with $n = \dim H$ eigenvectors all
orthogonal to each other
\epru

This theorem has several extensions to the case where the vector space is
of infinite dimension. 
Later in chapter \ref{ecuaciones_elipticas} we will see one of them.

Note that in this basis $\ve{A}$ is diagonal and therefore we have
\bcor
Every self-adjoint operator is diagonalizable
\ecor
%
Also note that if $\ve{u}$ is an eigenvector of $\ve{A}$ self-adjoint,
with eigenvalue $\lambda$, then,
\begin{equation} 
  \bar{\lambda} \langle \ve{u},\ve{u} \rangle = \langle \ve{A}(\ve{u}),\ve{u} \rangle
                               = \langle \ve{u},\ve{A}(\ve{u})\rangle
                               = \lambda \langle \ve{u},\ve{u} \rangle
\end{equation}
and therefore $\bar{\lambda} = \lambda$, that is,
    
\blem
The eigenvalues of a self-adjoint operator are real
\elem

Let's see what the condition of Hermiticity means in terms
of the components of the operator in an orthonormal basis.
Let $\ve{A}$ be a self-adjoint operator and let 
$\{\ve{e}_i\}$, $i=1,\dots,n$
be an orthonormal basis of the space where it acts.
We have that $\langle \ve{A}(\ve{e}_i),\ve{e}_j \rangle = \langle \ve{e}_i,\ve{A}(\ve{e}_j) \rangle$
and therefore, noting that 
$\ve{I} = \sum_{i=1}^{n} \ve{e}_i\ve{\theta}^i$,
we obtain,
\begin{eqnarray}
  0 &=& \langle \sum_{k=1}^{n} \ve{e}_k\ve{\theta}^k(\ve{A}(\ve{e}_i)),\ve{e}_j \rangle
       - \langle \ve{e}_i,\sum_{l=1}^{n} \ve{e}_l\ve{\theta}^l(\ve{A}(\ve{e}_j)) \rangle \nn
    &=& \sum_{k=1}^{n} \bar{A}^k{}_i \langle \ve{e}_k,\ve{e}_j \rangle
       -\sum_{l=1}^{n} A^l{}_j \langle \ve{e}_i,\ve{e}_l \rangle \nn
    &=& \sum_{k=1}^{n} \bar{A}^k{}_i \delta_{kj} 
       - \sum_{l=1}^{n} A^l{}_j \delta_{li} \nn
    &=&  \bar{A}^j{}_i - A^i{}_j
\end{eqnarray}
%
from which we conclude that 

\begin{equation}
  \bar{A}^j{}_i = A^i{}_j
\end{equation}
%
so the transpose matrix is the complex conjugate of the original. In the case of real matrices, we see that the condition is that in that basis the matrix is equal to its transpose, which is usually denoted by saying that the matrix is symmetric.

An interesting property of self-adjoint operators is that their norm is equal to the supremum of the magnitudes of their eigenvalues. Since the calculation demonstrating this will be used later in the course, we provide a demonstration of this fact below.

\blem 
If $\ve{A}$ is self-adjoint then $\|\ve{A}\| = \sup\{|\lambda_i|\}$.
\elem

\bpru 
Let $F(\ve{u}) := \langle \ve{A}(\ve{u}),\ve{A}(\ve{u}) \rangle$
defined on the sphere $\|\ve{u}\| = 1$. Since this set is compact
(here we are using the fact that the space is finite-dimensional)
it has a maximum which we will denote by $\ve{u}_0$. Note then that
$F(\ve{u}_0) := \|\ve{A}\|^2$.
Since $F(\ve{u})$ is differentiable on the sphere, it must satisfy 
\begin{equation}
  \frac{d}{d\lambda} F(\ve{u}_0+ \lambda \ve{\delta u})|_{\lambda=0} =0,
\end{equation}
%
along any curve tangent to the sphere at the point $\ve{u}_0$, 
that is, for all 
$\ve{\delta u}$ such that $\langle \ve{u}_0,\ve{\delta u} \rangle =0$.
See figure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure



\begin{figure}[htbp]
  \begin{center}
    \resizebox{7cm}{!}{\myinput{Figure/m2_4}}
    \caption{Normal and tangent vectors to the sphere.}
    \label{fig:2_4}
  \end{center}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent But 
\begin{eqnarray}
  \frac{d}{d\lambda} F(\ve{u}_0+ \lambda \ve{\delta u})|_{\lambda=0} 
       &=&
       \frac{d}{d\lambda} \langle \ve{A}(\ve{u}_0 + \lambda \ve{\delta u}),
                          \ve{A}(\ve{u}_0 + \lambda \ve{\delta u}) \rangle 
                          |_{\lambda=0} \nn
       &=&
           \langle \ve{A}(\ve{\delta u}),\ve{A}(\ve{u}_0) \rangle 
           + \langle \ve{A}(\ve{u}_0),\ve{A}(\ve{\delta u}) \rangle  \nn
       &=&
           2\Re \langle \ve{A}^{\star}\ve{A} \ve{u}_0, \ve{\delta u} \rangle  \nn
       &=&
           0 \;\;\;\;\; \forall \ve{\delta u}, 
           \;\;\; \langle \ve{u}_0,\ve{\delta u} \rangle =0
\end{eqnarray}
%
Since $\ve{\delta u}$ is arbitrary in $\{\ve{u}_0\}^{\perp}$ this 
simply implies
\begin{equation}
  \ve{A}^{\star}\ve{A}(\ve{u}_0) \;\; \in \; 
       \{\ve{\delta u}\}^{\perp} = \{\ve{u}_0\}^{\perp \perp} = \{\ve{u_0}\}.
\end{equation}
%
and therefore there will exist $\alpha \in \Complex$ such that
\begin{equation}
  \ve{A}^{\star}\ve{A}(\ve{u}_0) = \alpha \ve{u}_0.
\end{equation}
%
Taking the inner product with $\ve{u}_0$ we get,
\begin{eqnarray}
  \langle \ve{A}^{\star}\ve{A}(\ve{u}_0),\ve{u}_0 \rangle  
     &=& \bar{\alpha} \langle \ve{u}_0,\ve{u}_0 \rangle  \nn
     &=& \langle \ve{A}(\ve{u}_0),\ve{A}(\ve{u}_0) \rangle \nn
     &=& \|\ve{A}\|^2
\end{eqnarray}
%
and therefore we have that 
$\alpha = \bar{\alpha} = \|\ve{A}\|^2$.

Now let $\ve{v} := \ve{A} \ve{u}_0 - \|\ve{A}\|\ve{u}_0$,
then, now using that $\ve{A}$ is self-adjoint, we have,
\begin{eqnarray}
  \ve{A}(\ve{v}) &=& \ve{A}\ve{A}(\ve{u}_0) - \|\ve{A}\|\ve{A}(\ve{u}_0) \nn
                 &=& \ve{A}^{\star}\ve{A}(\ve{u}_0) - \|\ve{A}\|\ve{A}(\ve{u}_0) \nn
                 &=& \|\ve{A}\|^2 \ve{u}_0 - \|\ve{A}\|\ve{A}(\ve{u}_0) \nn
                 &=& \|\ve{A}\|(\|\ve{A}\|\ve{u}_0 - \ve{A}(\ve{u}_0)) \nn
                 &=& - \|\ve{A}\|\ve{v},
\end{eqnarray}
%
therefore either $\ve{v}$ is an eigenvector of $\ve{A}$, with eigenvalue 
$\lambda = - \|\ve{A}\|$, or $\ve{v} = 0$, in which case $\ve{u}_0$ is an
eigenvector of $\ve{A}$ with eigenvalue $\lambda = \|\ve{A}\|$
\epru






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Unitary Operators}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Another subclass of linear operators that appears very often in physics
when there is a privileged inner product is that of 
\textbf{unitary operators}~\index{operadores!unitarios},
that is, those such that their action preserves the inner product,
\begin{equation}
  \label{eq:operadores_unitarios}
  \langle \ve{U}(\ve{u}), \ve{U}(\ve{v}) \rangle  = \langle \ve{u},\ve{v} \rangle , 
                        \;\;\; \forall \ve{u}, \ve{v} \;\in H.
\end{equation}
The most typical case of a unitary operator is a transformation 
that sends one orthonormal basis to another. Usual examples are rotations 
in $\ren$.

\noindent
We also observe that 
$\|\ve{U}\| = \sup_{\|\ve{v}\|=1}\{\|\ve{U}(\ve{v})\|\} = 1$.

\noindent Note that 
\begin{equation}
  \langle \ve{U}(\ve{u}), \ve{U}(\ve{v}) \rangle  = \langle \ve{U}^{\star}\ve{U}(\ve{u}),\ve{v} \rangle 
                                   = \langle \ve{u},\ve{v} \rangle ,
                                   \;\;\; \forall \ve{u}, \ve{v} \;\in H,
\end{equation}
that is,
\begin{equation}
  \ve{U}^{\star}\ve{U} = \ve{I}
\end{equation}
and therefore,
\begin{equation}
  \ve{U}^{-1} = \ve{U}^{\star}.
\end{equation}

Let's see what the eigenvalues of a unitary operator $\ve{U}$ are.
Let $\ve{v}_1$ be an eigenvector of $\ve{U}$ (we know it has at least one),
then,
\begin{eqnarray}
  \langle \ve{U}(\ve{v}_1), \ve{U}(\ve{v}_1) \rangle  
               &=& \lambda_1 \bar{\lambda}_1 \langle \ve{v}_1,\ve{v}_1 \rangle  \nn
               &=& \langle \ve{v}_1,\ve{v}_1 \rangle 
\end{eqnarray}
%
and therefore $\lambda_1 = e^{i \theta_1}$ for some angle $\theta_1$.

If the operator $\ve{U}$ represents a non-trivial rotation in $\re^3$, then, given that we have an 
odd number of eigenvalues, there will be one that is real, the other two complex conjugates of each other.
The eigenvector corresponding to the real eigenvalue defines the axis that remains fixed in that rotation.
If we have more than one eigenvalue, then their corresponding eigenvectors
are orthogonal, indeed, let $\ve{v}_1$ and $\ve{v}_2$ be two eigenvectors
then
\begin{eqnarray}
  \langle \ve{U}(\ve{v}_1), \ve{U}(\ve{v}_2) \rangle 
               &=&  \bar{\lambda}_1 \lambda_2 \langle \ve{v}_1,\ve{v}_2 \rangle \nn
               &=& \langle \ve{v}_1,\ve{v}_2 \rangle 
\end{eqnarray}
%
and therefore if $\lambda_1 \neq \lambda_2$ we must have 
$\langle \ve{v}_1,\ve{v}_2 \rangle =0$.

\ejer:
Show that if $\ve{A}$ is a self-adjoint operator, then
$\ve{U} := e^{i\ve{A}}$ is a unitary operator.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\recu{
\noi\yaya{Equivalence Relations.}
\espa

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\defi: An {\bf equivalence relation}, $\approx$, between elements of a
set $X$ is a relation that satisfies the following conditions:

\begin{enumerate}

\item Reflexive: If $x \in X$, then $x \approx x$. 
\item Symmetric: If $x,x' \in X$ and $x \approx x'$, then $x' \approx x$.
\item Transitive: If $x, x',x'' \in X$, $x \approx x'$ and $x' \approx x''$, 
then $x \approx x''$.
\end{enumerate}

Note that the first property ensures that each element of $X$
satisfies an equivalence relation with some element of $X$, in
this case with itself. Equivalence relations often appear in physics,
essentially when we use a mathematical entity to describe a physical
process that has superfluous parts with respect to this process and
therefore we would like to ignore them.
This is achieved by declaring two entities that describe the same phenomenon
as equivalent entities.

\ejem: Let $X$ be the set of real numbers and let $x \approx y$
if and only if there exists an integer $n$ such that $x = n + y$, this
is clearly an equivalence relation. This is used when 
we are interested in describing something using the straight line but that in
reality should be described using a circle of unit circumference.


Given an equivalence relation in a set, we can group the
elements of this set into {\bf equivalence classes} of elements, that is,
into subsets where all their elements are equivalent to each other
and there is no element outside this subset that is
equivalent to any of the elements of the subset.
(If $X$ is the set, $Y \subset X$ is one of its equivalence classes,
and if $y \in Y$, then $y \approx y'$ if and only if $y' \in Y$.)

}
\recu{

The fundamental property of equivalence relations is the following.

\bteo
An equivalence relation in a set $X$ allows regrouping
its elements into equivalence classes such that each element of $X$
is in one and only one of these.
\eteo


\pru: 
Let $x \in X$ and $Y$ be the subset of all elements of $X$
equivalent to $x$. Let's see that this subset is an equivalence class.
Let $y$ and $y'$ be two elements of $Y$, that is, $y \approx x$ and $y'
\approx x$, but by the transitivity property, $y \approx y'$.
If $y \in Y$ and $z \notin Y$, then $y \napprox z$, because otherwise
$z$ would be equivalent to $x$ and therefore would be in $Y$. Finally, note that by reflexivity, $x$ is also in $Y$.
It only remains to see that if $y$ is in $Y$ and also in another equivalence class,
say $Z$, then $Y = Z$. Since $y \in Y$, then $y$ is equivalent to
every element of $Y$, and since $y \in Z$, then $y$ is equivalent to every
element of $Z$, but by transitivity, every element of $Y$
is equivalent to every element of $Z$, but since these are equivalence classes
and therefore each contains all its equivalent elements,
both must coincide.

\espa
\ejer: What are the equivalence classes of the previous examples?
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\section{Problems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bpro
Let the operator $\ve{A}: V \to V$ where $\dim V=n$, such that $\ve{A}\ve{x} = \lambda \ve{x}$. Calculate $\det{\ve{A}}$ and $tr\ve{A}$.
\epro

\bpro
Let $V=\re^3$ and $\ve{x}$ be any non-zero vector. Find geometrically and analytically the quotient space $V/W_{\ve{x}}$, where 
$W_{\ve{x}}$ is the space generated by $\ve{x}$. Take another vector, $\ve{x}'$,
linearly independent of the first and now calculate 
$V/W_{(\ve{x},\ve{x}')}$.
\epro

\bpro
The norm on operators is defined as:
\beq
\|A\|_{\cL}=\mbox{max}_{\|\ve x\|_V=1}\|A(\ve x)\|_V.
\eeq

Find the norms of the following operators, given by their
matrix representation with respect to a basis and where the norm in the 
vector space is the Euclidean norm with respect to that basis. 

a)
\begin{equation}
  \left(
    \begin{array}{cc}
      3 & 5  \\
      4 & 1 
    \end{array}
  \right)
\end{equation}

b)
\begin{equation}
  \left(
    \begin{array}{ccc}
      3 & 5 & 2 \\
      4 & 1 & 7 \\
      8 & 3 & 2
    \end{array}
  \right)
\end{equation}

\epro


\bpro
Let $V$ be any vector space and let $\|\cdot\|$ be a Euclidean norm in that
space. The Hilbert-Schmidt norm of an operator is defined as:
\begin{equation}
  \label{eq:Hilbert_Schmidt}
  \|\ve{A}\|_{HS}^2 = \sum_{i,j=1}^n |A^j{}_i|^2.
\end{equation}
where the basis used has been orthonormal with respect to the Euclidean norm.

a) Show that this is a norm.

b) Show that $\|\ve{A}\|_{\cL} \leq \|\ve{A}\|_{HS}$.

c) Show that $\sum_{j=1}^n |A^j{}_k|^2 \leq \|\ve{A}\|_{\cL}^2\;\;\mbox{for each}\; k$. Therefore {\small $\|\ve{A}\|_{HS}^2 \leq n \|\ve{A}\|_{\cL}^2$}, and the two norms
are equivalent.

\noi Hint: use that 
$\ve{\theta}^j(\ve{A}(\ve{u})) = \ve{\theta}^j(\ve{A})(\ve{u})$ 
and then that
$|\ve{\theta}(\ve{u})| \leq \|\ve{\theta}\| \|\ve{u}\|$.
\epro

\bpro
Calculate the eigenvalues and eigenvectors of the following matrices:

a)
\begin{equation}
  \left(
    \begin{array}{cc}
      3 & 6  \\
      4 & 1 
    \end{array}
  \right)
\end{equation}

b)
\begin{equation}
  \left(
    \begin{array}{cc}
      3 & 6  \\
      0 & 1 
    \end{array}
  \right)
\end{equation}

c)
\begin{equation}
  \left(
    \begin{array}{ccc}
      2 & 4 & 2 \\
      4 & 1 & 0 \\
      3 & 3 & 1
    \end{array}
  \right)
\end{equation}
\epro

\bpro
Bring the following matrices to upper triangular form:
Note: From the transformation of the bases.

a)
\begin{equation}
  \left(
    \begin{array}{cc}
      3 & 4  \\
      2 & 1 
    \end{array}
  \right)
\end{equation}

b)
\begin{equation}
  \left(
    \begin{array}{ccc}
      2 & 4 & 2 \\
      4 & 1 & 0 \\
      3 & 3 & 1
    \end{array}
  \right)
\end{equation}

c)
\begin{equation}
  \left(
    \begin{array}{ccc}
      1 & 4 & 3 \\
      4 & 4 & 0 \\
      3 & 3 & 1
    \end{array}
  \right)
\end{equation}

\epro

\bpro
Show again that $\det e^{\ve{A}} = e^{tr\ve{A}}$. Hint: express the matrix
representation of $\ve{A}$ in a basis where it has the Jordan canonical form.
Alternatively, use a basis where $\ve{A}$ is upper triangular and see that
the product of upper triangular matrices gives an upper triangular matrix and
therefore the exponential of an upper triangular matrix is also
upper triangular.
\epro


\recubib{This chapter is based on the following books: 
\cite{Geroch}, \cite{Arnold}, \cite{Lefschetz} and \cite{TaylorI}. The following are also of interest, \cite{Lang_LA} and \cite{Lax}.
Linear algebra is one of the largest and most prolific areas of mathematics, especially when dealing with infinite-dimensional spaces, which is usually called real analysis and operator theory. In my personal experience, most problems end up reducing to an algebraic problem and one feels that progress has been made when that problem can be solved.}
 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "apu_tot.tex"
%%% End: